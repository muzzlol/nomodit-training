{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets tqdm python-dotenv google-api-core --quiet\n",
    "%pip install -U -q \"google-genai>=1.10.0\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import concurrent.futures\n",
    "import threading \n",
    "from tqdm.notebook import tqdm\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") # always restart kernel to make changes to this take effect\n",
    "MODEL_ID = \"gemini-2.5-flash-lite-preview-06-17\"\n",
    "OUTPUT_FILE = \"./data/train_reasoning.jsonl\"\n",
    "FREE_LIMIT = 1000\n",
    "MAX_CALLS_PER_RUN = 10\n",
    "\n",
    "# --- Concurrency Configuration ---\n",
    "logical_cores = os.cpu_count()\n",
    "MAX_WORKERS = logical_cores * 2 # threads making api calls to gemini. NOTE: this is a conservative estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "You are an AI language expert simulating the process of text editing. You will receive a source text (`src`) which contains an editing instruction followed by the text to be edited.\n",
    "\n",
    "Your task is to:\n",
    "1.  Carefully identify the editing instruction given at the beginning of the `src` text.\n",
    "2.  Analyze the original text segment that follows the instruction in `src`.\n",
    "3.  Apply the instruction step-by-step to the original text.\n",
    "4.  For each step or change you make, articulate the precise reasoning based on linguistic rules, the instruction's goal (e.g., grammar, clarity, simplification), or common writing conventions. Explain *why* a change is necessary or beneficial according to the instruction.\n",
    "5.  Your reasoning should demonstrate *how* to arrive at the corrected text, simulating the thought process of performing the edit yourself.\n",
    "\n",
    "**Crucially:** Your reasoning process should *not* refer to or imply knowledge of a pre-existing target or corrected text. Focus *only* on applying the instruction to the source text to derive the necessary changes.\n",
    "\n",
    "Your entire output must consist *only* of the step-by-step reasoning process, enclosed within `<think></think>` tags. Do not include the original instruction, the source text, the final corrected text, or any other text outside these tags.\n",
    "\n",
    "**Example Interaction:**\n",
    "\n",
    "**User Input (constructed from your dataset):**\n",
    "\n",
    "```\n",
    "Source (src): Fix grammar in this sentence: If engineers do not come up with new ideas, they cannot find best solution for the problems.\n",
    "Target (tgt): If engineers do not come up with new ideas, they cannot find the best solution for different problems.\n",
    "```\n",
    "\n",
    "**Your Expected Output (demonstrating the reasoning to *derive* the correction):**\n",
    "\n",
    "<think>\n",
    "1. Instruction analysis: The task is to \"Fix grammar\". This requires identifying and correcting grammatical errors in the sentence \"If engineers do not come up with new ideas, they cannot find best solution for the problems.\"\n",
    "2. Sentence segment analysis: The first clause \"If engineers do not come up with new ideas\" appears grammatically sound.\n",
    "3. Focus on the second clause: \"they cannot find best solution for the problems.\"\n",
    "4. Identify potential error 1: The noun phrase \"best solution\". \"Best\" is a superlative adjective. Grammatical rule: Superlative adjectives modifying singular countable nouns generally require a definite article (\"the\").\n",
    "5. Apply correction 1: Insert \"the\" before \"best solution\". The phrase becomes \"the best solution\".\n",
    "6. Identify potential error/improvement area 2: The phrase \"for the problems\". While grammatically plausible, using \"the\" implies specific, previously identified problems. The context seems general. Improving grammatical flow or clarity might involve adjusting this.\n",
    "7. Consider alternatives for \"the problems\": \"for problems\" (too general?), \"for specific problems\" (adds info), \"for different problems\" (suggests variety, often fits general statements well).\n",
    "8. Apply correction 2: Replace \"the problems\" with \"different problems\" for better contextual fit and naturalness, aligning with the goal of general grammatical improvement and clarity often included in \"Fix grammar\" tasks.\n",
    "9. Synthesized correction based on reasoning: The corrected clause is \"they cannot find the best solution for different problems.\" The full sentence derived from applying the grammar fixes is \"If engineers do not come up with new ideas, they cannot find the best solution for different problems.\"\n",
    "</think>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 69071 rows\n"
     ]
    }
   ],
   "source": [
    "# --- Main Processing Logic ---\n",
    "if not gemini_api_key:\n",
    "    print(\"Error: GEMINI_API_KEY not found in environment variables.\")\n",
    "else:\n",
    "    # Initialize client\n",
    "    client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        train_ds = load_dataset(\"grammarly/coedit\", split=\"train\", cache_dir=\"./data\")\n",
    "        print(f\"Dataset loaded: {len(train_ds)} rows\")\n",
    "        train_ds = train_ds.shuffle(seed=42)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load dataset. {e}\")\n",
    "        train_ds = None # Ensure loop doesn't run\n",
    "\n",
    "# Global flag for controlled shutdown\n",
    "shutdown_requested = threading.Event()\n",
    "\n",
    "# Custom exception for controlled shutdown\n",
    "class APIShutdownException(Exception):\n",
    "    \"\"\"Exception raised to trigger a controlled shutdown of API processing.\"\"\"\n",
    "    pass        \n",
    "\n",
    "# --- Helper Function to Load Processed IDs ---\n",
    "\n",
    "def load_processed_ids(filename):\n",
    "    processed = set()\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    if '_id' in data:\n",
    "                        processed.add(data['_id'])\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping invalid JSON line in {filename}\")\n",
    "    print(f\"Loaded {len(processed)} processed IDs from {filename}\")\n",
    "    return processed\n",
    "\n",
    "# --- Worker Function ---\n",
    "\n",
    "def process_item(item, client, sys_prompt):\n",
    "    \"\"\"Processes a single dataset item to get reasoning.\"\"\"\n",
    "    item_id = item['_id']\n",
    "    \n",
    "    if shutdown_requested.is_set():\n",
    "        return {\"_id\": item_id, \"error\": \"Skipped due to shutdown request\"}\n",
    "\n",
    "    item_src = item['src']\n",
    "    item_tgt = item['tgt']\n",
    "    user_prompt = f\"Source (src): {item_src}\\nTarget (tgt): {item_tgt}\"\n",
    "\n",
    "    try:\n",
    "        # ---  API Call ---\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=user_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "            system_instruction=sys_prompt,\n",
    "                    thinking_config=types.ThinkingConfig(\n",
    "                        thinking_budget=600 # 0 means no thinking None means no limit to thinking it can take however many tokens it wants.\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # --- Process Response ---\n",
    "        reasoning_text = response.text\n",
    "\n",
    "        if reasoning_text and reasoning_text.strip().startswith(\"```xml\") and reasoning_text.strip().endswith(\"```\"):\n",
    "            reasoning_text = reasoning_text[6:-3].strip() # removing the xml codefences\n",
    "\n",
    "        if reasoning_text.strip().startswith(\"<think>\") and reasoning_text.strip().endswith(\"</think>\"):\n",
    "            return {\"_id\": item_id, \"reasoning\": reasoning_text.strip()}\n",
    "        else:\n",
    "            reasoning_text = \"<think>\" + reasoning_text + \"</think>\"\n",
    "            return {\"_id\": item_id, \"reasoning\": reasoning_text.strip()}\n",
    "    except Exception as e:\n",
    "        if hasattr(e, 'code') and 400 <= e.code < 600:\n",
    "            error_msg = f\"Critical HTTP error {e.code}: {e}\"\n",
    "            print(f\"\\n{error_msg}\")\n",
    "            # Signal all threads to stop\n",
    "            shutdown_requested.set()\n",
    "            # Propagate the exception to interrupt processing\n",
    "            raise APIShutdownException(error_msg)\n",
    "        \n",
    "        # --- Handle API or Processing Errors ---\n",
    "        print(f\"\\nError processing ID {item_id}: {e}\")\n",
    "        return {\"_id\": item_id, \"error\": str(e)} # Return error indicator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 56739 processed IDs from ./data/train_reasoning.jsonl\n",
      "Filtering items to process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a37d4a323df4fcf9c3ec5a8162e2d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/69071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12332 items needing processing.\n",
      "Entering Free tier. Processing 10 items sequentially...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca91c8e4c824fdeb3cac4887aeefefc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing items:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item 1 of 10\n",
      "Processing item 2 of 10\n",
      "Processing item 3 of 10\n",
      "Processing item 4 of 10\n",
      "Processing item 5 of 10\n",
      "Processing item 6 of 10\n",
      "Processing item 7 of 10\n",
      "Processing item 8 of 10\n",
      "Processing item 9 of 10\n",
      "Processing item 10 of 10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main processing section ---\n",
    "if train_ds:\n",
    "    processed_ids = load_processed_ids(OUTPUT_FILE)\n",
    "    api_calls_made_this_run = 0\n",
    "    file_lock = threading.Lock()\n",
    "    items_to_process = []\n",
    "\n",
    "    print(\"Filtering items to process...\")\n",
    "    for item in tqdm(train_ds, desc=\"Filtering\"):\n",
    "        if item['_id'] not in processed_ids:\n",
    "            items_to_process.append(item)\n",
    "    print(f\"Found {len(items_to_process)} items needing processing.\")\n",
    "\n",
    "if MAX_CALLS_PER_RUN <= FREE_LIMIT:\n",
    "    print(f\"Entering Free tier. Processing {MAX_CALLS_PER_RUN} items sequentially...\")\n",
    "    for i in tqdm(range(MAX_CALLS_PER_RUN), desc=\"Processing items\", total=MAX_CALLS_PER_RUN):\n",
    "        print(f\"Processing item {i+1} of {MAX_CALLS_PER_RUN}\")\n",
    "        time.sleep(1) # to avoid rpm rate limit (10/min if you go over you incur costs)\n",
    "        response = process_item(items_to_process[i], client, sys_prompt)\n",
    "        if response and 'error' not in response:\n",
    "            with open(OUTPUT_FILE, 'a', encoding='utf-8') as outfile:\n",
    "                json_line = json.dumps(response)\n",
    "                outfile.write(json_line + '\\n')\n",
    "                outfile.flush()\n",
    "                processed_ids.add(response['_id'])\n",
    "        else:\n",
    "            print(f\"Error processing item {i} of {MAX_CALLS_PER_RUN}: {response['error'] if response else 'Unknown error'}\")\n",
    "\n",
    "elif MAX_CALLS_PER_RUN > FREE_LIMIT:\n",
    "    print(f\"Entering Paid tier.Processing {MAX_CALLS_PER_RUN} items concurrently...\")\n",
    "    # Process items concurrently\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as outfile:\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                # stores ids of items that will be processed\n",
    "                futures = {}\n",
    "                \n",
    "                # Submit tasks only up to the daily limit\n",
    "                num_to_submit = min(len(items_to_process), MAX_CALLS_PER_RUN - api_calls_made_this_run)\n",
    "                print(f\"Submitting {num_to_submit} tasks to the executor...\")\n",
    "\n",
    "                submitted_count = 0\n",
    "                for i in range(len(items_to_process)):\n",
    "                    if submitted_count >= num_to_submit or shutdown_requested.is_set():\n",
    "                        break  # Stop submitting if we hit the limit or shutdown requested\n",
    "                    item = items_to_process[i]\n",
    "                    future = executor.submit(process_item, item, client, sys_prompt)\n",
    "                    futures[future] = item['_id']\n",
    "                    submitted_count += 1\n",
    "\n",
    "                print(f\"Tasks submitted ({submitted_count}). Waiting for results...\")\n",
    "\n",
    "                # Process results as they complete\n",
    "                completed_futures = []\n",
    "\n",
    "                for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing Results\"):\n",
    "                    if shutdown_requested.is_set():\n",
    "                        print(\"\\nShutdown requested. Canceling remaining tasks...\")\n",
    "                        for f in futures:\n",
    "                            if f not in completed_futures and not f.done():\n",
    "                                f.cancel()\n",
    "                        break\n",
    "        \n",
    "                    completed_futures.append(future)\n",
    "                    item_id = futures[future]       \n",
    "                    \n",
    "                    try:\n",
    "                        result_data = future.result()\n",
    "                        \n",
    "                        if result_data and \"error\" not in result_data:\n",
    "                            with file_lock:\n",
    "                                if result_data['_id'] not in processed_ids:\n",
    "                                    json_line = json.dumps(result_data)\n",
    "                                    outfile.write(json_line + '\\n')\n",
    "                                    outfile.flush()\n",
    "                                    processed_ids.add(result_data['_id'])\n",
    "                                    api_calls_made_this_run += 1\n",
    "                                    \n",
    "                    except APIShutdownException:\n",
    "                        print(f\"Task for item {item_id} aborted due to shutdown request\")\n",
    "                    except Exception as exc:\n",
    "                        print(f'\\nItem ID {item_id} generated an exception: {exc}')\n",
    "                        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nKeyboard interrupt detected. Shutting down gracefully...\")\n",
    "            shutdown_requested.set()\n",
    "            \n",
    "    # Final status report\n",
    "    if shutdown_requested.is_set():\n",
    "        print(\"\\nProcessing terminated early due to errors or interruption.\")\n",
    "    else:\n",
    "        print(f\"\\nFinished processing batch. Made approximately {api_calls_made_this_run} successful API calls in this run.\")\n",
    "    \n",
    "    print(f\"Total processed IDs (including previous runs): {len(processed_ids)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No items to process\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the token counts (use 0 if Thoughts tokens is None or zero):\n",
      "\n",
      "Calculating using THINKING output price ($3.50 / 1M tokens)\n",
      "--- Cost Breakdown ---\n",
      "Input Cost:       $0.000120\n",
      "Output Cost:      $0.003507 (based on 1002 billable output tokens)\n",
      "----------------------\n",
      "Total Estimated Cost: $0.003627\n",
      "\n",
      "Estimated Cost for 10,000 calls: $36.27\n"
     ]
    }
   ],
   "source": [
    "# Pricing per 1 Million tokens\n",
    "INPUT_PRICE_PER_MILLION = 0.15\n",
    "OUTPUT_NON_THINKING_PRICE_PER_MILLION = 0.60\n",
    "OUTPUT_THINKING_PRICE_PER_MILLION = 3.50\n",
    "\n",
    "# --- Get Token Counts from User ---\n",
    "print(\"Enter the token counts (use 0 if Thoughts tokens is None or zero):\")\n",
    "\n",
    "prompt_tokens = 800\n",
    "thoughts_tokens = 393 # Assume 0 if None was the actual value\n",
    "output_tokens = 609\n",
    "\n",
    "# --- Calculate Costs ---\n",
    "\n",
    "# Input Cost\n",
    "input_cost = (prompt_tokens / 1_000_000) * INPUT_PRICE_PER_MILLION\n",
    "\n",
    "# Output Cost (depends on whether thinking tokens were generated)\n",
    "if thoughts_tokens > 0:\n",
    "    # Thinking was used - price applies to output + thoughts tokens\n",
    "    billable_output_tokens = output_tokens + thoughts_tokens\n",
    "    output_cost = (billable_output_tokens / 1_000_000) * OUTPUT_THINKING_PRICE_PER_MILLION\n",
    "    print(\"\\nCalculating using THINKING output price ($3.50 / 1M tokens)\")\n",
    "else:\n",
    "    # No thinking - price applies only to output tokens\n",
    "    billable_output_tokens = output_tokens\n",
    "    output_cost = (billable_output_tokens / 1_000_000) * OUTPUT_NON_THINKING_PRICE_PER_MILLION\n",
    "    print(\"\\nCalculating using NON-THINKING output price ($0.60 / 1M tokens)\")\n",
    "\n",
    "# Total Cost\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"--- Cost Breakdown ---\")\n",
    "print(f\"Input Cost:       ${input_cost:.6f}\")\n",
    "print(f\"Output Cost:      ${output_cost:.6f} (based on {billable_output_tokens} billable output tokens)\")\n",
    "print(\"----------------------\")\n",
    "print(f\"Total Estimated Cost: ${total_cost:.6f}\")\n",
    "\n",
    "# Example calculation for 10,000 identical calls\n",
    "num_calls = 10000\n",
    "total_cost_10k = total_cost * num_calls\n",
    "print(f\"\\nEstimated Cost for {num_calls:,} calls: ${total_cost_10k:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out other models or inspect api errors\n",
    "\n",
    "import sys\n",
    "\n",
    "prompt = \"\"\"\n",
    "    what is the capital of the moon?\n",
    "\"\"\"\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite-preview-06-17\",\n",
    "        # model=\"gemini-2.0-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "except Exception as e:\n",
    "    if 400 <= e.code < 600:\n",
    "        print(f\"Exiting due to HTTP error {e.code}: {e}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Moon does not have a capital.\n",
      "\n",
      "It's not a country or a political entity with a government. Currently, there are no permanent human settlements or established governing bodies on the Moon that would designate a capital city.\n",
      "\n",
      "However, in science fiction, you'll often find imagined lunar cities or bases, but these are purely fictional concepts!\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -- Uploading augmented dataset to hf --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69071 {'_id': '1', 'task': 'gec', 'src': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'tgt': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.'}\n",
      "1712 {'_id': '1', 'task': 'paraphrase', 'src': 'Paraphrase this sentence: Why are you arresting me?', 'tgt': 'Why am I being arrested?'}\n"
     ]
    }
   ],
   "source": [
    "# load the original coedit dataset\n",
    "\n",
    "og_coedit_train = load_dataset(\"grammarly/coedit\", split=\"train\")\n",
    "og_coedit_val = load_dataset(\"grammarly/coedit\", split=\"validation\")\n",
    "\n",
    "print(len(og_coedit_train), og_coedit_train[0])\n",
    "print(len(og_coedit_val), og_coedit_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '1', 'reasoning': '<think>\\n1.  **Instruction Analysis:** The goal is to \"Remove all grammatical errors\" from the provided text. I need to read the text carefully and identify any points that violate standard English grammar rules.\\n2.  **Source Text Analysis:** The text is \"For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\"\\n3.  **Sentence Structure Review:** The sentence starts with an introductory phrase \"For example\". The main subject is \"countries with a lot of deserts\". The main verb phrase is \"can [verb]\".\\n4.  **Identify Potential Error 1:** The phrase \"can terraform their desert to increase their habitable land\" appears structurally sound. \"Can\" is followed by the base form of the verb \"terraform\". The infinitive \"to increase\" explains the purpose.\\n5.  **Identify Potential Error 2:** The coordinating conjunction \"and\" connects the action \"terraform their desert to increase their habitable land\" with the phrase \"using irrigation to provide clean water to the desert.\" This creates a problem with parallel structure. The modal verb \"can\" governs the actions that follow. If both actions are things the countries *can* do, they should be presented in a parallel grammatical form after \"can\".\\n6.  **Analyze the parallelism issue:** The first action connected by \"can\" starts with the base verb \"terraform\". The second clause starts with the gerund phrase \"using irrigation\". To maintain parallelism after the modal \"can\", the second action should also start with a base form of a verb.\\n7.  **Determine the correct form for parallelism:** The intended meaning is that countries *can* \"terraform\" *and* *can* \"use\" irrigation. Therefore, the word \"using\" should be changed to the base form \"use\" to match the structure after \"can\".\\n8.  **Apply the correction:** Change \"using\" to \"use\". The phrase becomes \"and use irrigation to provide clean water to the desert.\"\\n9.  **Review the text with the correction:** \"For example, countries with a lot of deserts can terraform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\" This structure (\"can terraform... and use...\") is grammatically correct and parallel.\\n10. **Final Check:** Read the entire corrected sentence to ensure no other errors are present and that the flow is natural. \"For example, countries with a lot of deserts can terraform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\" The sentence is now grammatically sound.\\n</think>'}\n",
      "{'_id': '1', 'reasoning': '<think>\\n1.  **Instruction Analysis:** The instruction is to \"Paraphrase this sentence.\" This means rephrasing the original sentence, \"Why are you arresting me?\", to convey the same meaning but using different wording or sentence structure.\\n\\n2.  **Original Sentence Analysis:**\\n    *   \"Why are you arresting me?\"\\n    *   Subject: \"you\" (the person performing the arrest)\\n    *   Verb: \"are arresting\" (present continuous tense, active voice)\\n    *   Object: \"me\" (the person being arrested)\\n    *   Interrogative word: \"Why\" (inquiring about the reason)\\n\\n3.  **Paraphrasing Strategy 1: Change Voice (Active to Passive):**\\n    *   One common paraphrasing technique is to change the voice of the verb. The original sentence is in the active voice (\"you are arresting me\").\\n    *   To change to passive voice, the object of the active sentence (\"me\") becomes the subject of the passive sentence.\\n    *   The verb form changes to \"to be\" + past participle of the main verb. The tense (present continuous) should be maintained.\\n    *   The original subject (\"you\") can be omitted or introduced with \"by\" (e.g., \"by you\").\\n\\n4.  **Applying Passive Voice Transformation:**\\n    *   Original object \"me\" becomes the new subject \"I\".\\n    *   The verb \"are arresting\" (present continuous active) needs to be transformed into present continuous passive.\\n        *   Present continuous of \"to be\": \"am being\" (since the new subject is \"I\").\\n        *   Past participle of \"arrest\": \"arrested\".\\n        *   So, the passive verb phrase is \"am being arrested\".\\n    *   The interrogative \"Why\" remains at the beginning.\\n    *   The original subject \"you\" is implicit in the context of the question. In a passive construction like this, asking *why* one is being arrested implies the question is directed at the arresters, so \"by you\" is often omitted for conciseness and naturalness.\\n\\n5.  **Constructing the Paraphrased Sentence:**\\n    *   Combining the elements: \"Why\" + \"I\" + \"am being arrested\" + \"?\"\\n    *   This results in: \"Why am I being arrested?\"\\n\\n6.  **Meaning Check:**\\n    *   Original: \"Why are you arresting me?\" - Focuses on the action of \"you\".\\n    *   Paraphrased: \"Why am I being arrested?\" - Focuses on the state of \"me\" (being arrested).\\n    *   Both sentences ask for the reason for the arrest. The core meaning is preserved. The paraphrase shifts the focus from the agent (the arrester) to the recipient of the action (the one being arrested), which is a valid way to rephrase while maintaining the core inquiry.\\n\\n7.  **Final Paraphrased Sentence (derived from steps):** \"Why am I being arrested?\"\\n</think>'}\n",
      "Train reasoning traces: 56736 examples\n",
      "Val reasoning traces: 1712 examples\n"
     ]
    }
   ],
   "source": [
    "# load the reasoning traces\n",
    "train_reasoning_traces = load_dataset(\"json\", data_files=\"./data/train_reasoning.jsonl\", split=\"train\")\n",
    "val_reasoning_traces = load_dataset(\"json\", data_files=\"./data/val_reasoning.jsonl\", split=\"train\")\n",
    "\n",
    "print(train_reasoning_traces[0])\n",
    "print(val_reasoning_traces[0])\n",
    "\n",
    "print(f\"Train reasoning traces: {len(train_reasoning_traces)} examples\")\n",
    "print(f\"Val reasoning traces: {len(val_reasoning_traces)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reasoning_map = {row['_id']: row['reasoning'] for row in train_reasoning_traces}\n",
    "val_reasoning_map = {row['_id']: row['reasoning'] for row in val_reasoning_traces}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding reasoning traces to train split...\n",
      "Adding reasoning traces to validation split...\n",
      "Reasoning coverage:\n",
      "Train: 56736/69071 (82.1%)\n",
      "Val: 1712/1712 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "def add_reasoning(dataset, reasoning_map):\n",
    "    dataset[\"reasoning\"] = reasoning_map.get(dataset[\"_id\"], None)\n",
    "    return dataset\n",
    "\n",
    "print(\"\\nAdding reasoning traces to train split...\")\n",
    "train_with_reasoning = og_coedit_train.map(\n",
    "    lambda x: add_reasoning(x, train_reasoning_map),\n",
    "    desc=\"Adding reasoning to train\"\n",
    ")\n",
    "\n",
    "print(\"Adding reasoning traces to validation split...\")\n",
    "val_with_reasoning = og_coedit_val.map(\n",
    "    lambda x: add_reasoning(x, val_reasoning_map),\n",
    "    desc=\"Adding reasoning to validation\"\n",
    ")\n",
    "\n",
    "\n",
    "train_with_reasoning_count = sum(1 for x in train_with_reasoning if x['reasoning'] is not None)\n",
    "val_with_reasoning_count = sum(1 for x in val_with_reasoning if x['reasoning'] is not None)\n",
    "\n",
    "print(\"Reasoning coverage:\")\n",
    "print(f\"Train: {train_with_reasoning_count}/{len(train_with_reasoning)} ({train_with_reasoning_count/len(train_with_reasoning)*100:.1f}%)\")\n",
    "print(f\"Val: {val_with_reasoning_count}/{len(val_with_reasoning)} ({val_with_reasoning_count/len(val_with_reasoning)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating DatasetDict...\n",
      "\n",
      "Dataset preview:\n",
      "Train example: {'_id': '1', 'task': 'gec', 'src': 'Remove all grammatical errors from this text: For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.', 'tgt': 'For example, countries with a lot of deserts can transform their desert to increase their habitable land and use irrigation to provide clean water to the desert.', 'reasoning': '<think>\\n1.  **Instruction Analysis:** The goal is to \"Remove all grammatical errors\" from the provided text. I need to read the text carefully and identify any points that violate standard English grammar rules.\\n2.  **Source Text Analysis:** The text is \"For example, countries with a lot of deserts can terraform their desert to increase their habitable land and using irrigation to provide clean water to the desert.\"\\n3.  **Sentence Structure Review:** The sentence starts with an introductory phrase \"For example\". The main subject is \"countries with a lot of deserts\". The main verb phrase is \"can [verb]\".\\n4.  **Identify Potential Error 1:** The phrase \"can terraform their desert to increase their habitable land\" appears structurally sound. \"Can\" is followed by the base form of the verb \"terraform\". The infinitive \"to increase\" explains the purpose.\\n5.  **Identify Potential Error 2:** The coordinating conjunction \"and\" connects the action \"terraform their desert to increase their habitable land\" with the phrase \"using irrigation to provide clean water to the desert.\" This creates a problem with parallel structure. The modal verb \"can\" governs the actions that follow. If both actions are things the countries *can* do, they should be presented in a parallel grammatical form after \"can\".\\n6.  **Analyze the parallelism issue:** The first action connected by \"can\" starts with the base verb \"terraform\". The second clause starts with the gerund phrase \"using irrigation\". To maintain parallelism after the modal \"can\", the second action should also start with a base form of a verb.\\n7.  **Determine the correct form for parallelism:** The intended meaning is that countries *can* \"terraform\" *and* *can* \"use\" irrigation. Therefore, the word \"using\" should be changed to the base form \"use\" to match the structure after \"can\".\\n8.  **Apply the correction:** Change \"using\" to \"use\". The phrase becomes \"and use irrigation to provide clean water to the desert.\"\\n9.  **Review the text with the correction:** \"For example, countries with a lot of deserts can terraform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\" This structure (\"can terraform... and use...\") is grammatically correct and parallel.\\n10. **Final Check:** Read the entire corrected sentence to ensure no other errors are present and that the flow is natural. \"For example, countries with a lot of deserts can terraform their desert to increase their habitable land and use irrigation to provide clean water to the desert.\" The sentence is now grammatically sound.\\n</think>'}\n",
      "Validation example: {'_id': '1', 'task': 'paraphrase', 'src': 'Paraphrase this sentence: Why are you arresting me?', 'tgt': 'Why am I being arrested?', 'reasoning': '<think>\\n1.  **Instruction Analysis:** The instruction is to \"Paraphrase this sentence.\" This means rephrasing the original sentence, \"Why are you arresting me?\", to convey the same meaning but using different wording or sentence structure.\\n\\n2.  **Original Sentence Analysis:**\\n    *   \"Why are you arresting me?\"\\n    *   Subject: \"you\" (the person performing the arrest)\\n    *   Verb: \"are arresting\" (present continuous tense, active voice)\\n    *   Object: \"me\" (the person being arrested)\\n    *   Interrogative word: \"Why\" (inquiring about the reason)\\n\\n3.  **Paraphrasing Strategy 1: Change Voice (Active to Passive):**\\n    *   One common paraphrasing technique is to change the voice of the verb. The original sentence is in the active voice (\"you are arresting me\").\\n    *   To change to passive voice, the object of the active sentence (\"me\") becomes the subject of the passive sentence.\\n    *   The verb form changes to \"to be\" + past participle of the main verb. The tense (present continuous) should be maintained.\\n    *   The original subject (\"you\") can be omitted or introduced with \"by\" (e.g., \"by you\").\\n\\n4.  **Applying Passive Voice Transformation:**\\n    *   Original object \"me\" becomes the new subject \"I\".\\n    *   The verb \"are arresting\" (present continuous active) needs to be transformed into present continuous passive.\\n        *   Present continuous of \"to be\": \"am being\" (since the new subject is \"I\").\\n        *   Past participle of \"arrest\": \"arrested\".\\n        *   So, the passive verb phrase is \"am being arrested\".\\n    *   The interrogative \"Why\" remains at the beginning.\\n    *   The original subject \"you\" is implicit in the context of the question. In a passive construction like this, asking *why* one is being arrested implies the question is directed at the arresters, so \"by you\" is often omitted for conciseness and naturalness.\\n\\n5.  **Constructing the Paraphrased Sentence:**\\n    *   Combining the elements: \"Why\" + \"I\" + \"am being arrested\" + \"?\"\\n    *   This results in: \"Why am I being arrested?\"\\n\\n6.  **Meaning Check:**\\n    *   Original: \"Why are you arresting me?\" - Focuses on the action of \"you\".\\n    *   Paraphrased: \"Why am I being arrested?\" - Focuses on the state of \"me\" (being arrested).\\n    *   Both sentences ask for the reason for the arrest. The core meaning is preserved. The paraphrase shifts the focus from the agent (the arrester) to the recipient of the action (the one being arrested), which is a valid way to rephrase while maintaining the core inquiry.\\n\\n7.  **Final Paraphrased Sentence (derived from steps):** \"Why am I being arrested?\"\\n</think>'}\n"
     ]
    }
   ],
   "source": [
    "# create dataset to upload onto huggingface\n",
    "print(\"\\nCreating DatasetDict...\")\n",
    "coedit_w_reasoning = DatasetDict({\n",
    "    \"train\": train_with_reasoning,\n",
    "    \"validation\": val_with_reasoning\n",
    "})\n",
    "\n",
    "print(\"\\nDataset preview:\")\n",
    "print(\"Train example:\", coedit_w_reasoning[\"train\"][0])\n",
    "print(\"Validation example:\", coedit_w_reasoning[\"validation\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logged in\n"
     ]
    }
   ],
   "source": [
    "# initialize write token for huggingface\n",
    "import os\n",
    "token = os.environ.get(\"HF_WRITE_TOKEN\")\n",
    "login(token=token)\n",
    "print('logged in')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38feb81433a44408c05187b0c6574a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b6b0c1ee945471eaf89c64593c65ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/70 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300d0ad702cc42098ad3997af796dc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdd3b84e7eb4924ba202f570b212bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/muzzz/coedit-w-reasoning-traces/commit/b8f25c337f2c136f789c8028fcb3484090aa42c4', commit_message='Added reasoning traces to coedit dataset', commit_description='', oid='b8f25c337f2c136f789c8028fcb3484090aa42c4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/muzzz/coedit-w-reasoning-traces', endpoint='https://huggingface.co', repo_type='dataset', repo_id='muzzz/coedit-w-reasoning-traces'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_repo = \"muzzz/coedit-w-reasoning-traces\" # USERNAME/REPO_NAME\n",
    "\n",
    "coedit_w_reasoning.push_to_hub(\n",
    "    hf_repo,\n",
    "    private=True,\n",
    "    commit_message=\"Added reasoning traces to coedit dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
