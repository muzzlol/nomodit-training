{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets tqdm python-dotenv google-api-core --quiet\n",
    "%pip install -U -q \"google-genai>=1.10.0\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import concurrent.futures\n",
    "import threading \n",
    "import signal\n",
    "from tqdm.notebook import tqdm\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset\n",
    "from google.api_core import exceptions as core_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "load_dotenv()\n",
    "gemini_api_key = os.getenv(\"GEMINI_API_KEY\") # restart kernel to make changes to this take effect\n",
    "MODEL_ID = \"gemini-2.5-flash-preview-05-20\"\n",
    "OUTPUT_FILE = \"./data/train_reasoning.jsonl\"\n",
    "MAX_CALLS_PER_RUN = 499\n",
    "\n",
    "# --- Concurrency Configuration ---\n",
    "# recommend not going over 15-20 (conservative default)\n",
    "MAX_WORKERS = 15 # Number of concurrent API requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"\n",
    "You are an AI language expert simulating the process of text editing. You will receive a source text (`src`) which contains an editing instruction followed by the text to be edited.\n",
    "\n",
    "Your task is to:\n",
    "1.  Carefully identify the editing instruction given at the beginning of the `src` text.\n",
    "2.  Analyze the original text segment that follows the instruction in `src`.\n",
    "3.  Apply the instruction step-by-step to the original text.\n",
    "4.  For each step or change you make, articulate the precise reasoning based on linguistic rules, the instruction's goal (e.g., grammar, clarity, simplification), or common writing conventions. Explain *why* a change is necessary or beneficial according to the instruction.\n",
    "5.  Your reasoning should demonstrate *how* to arrive at the corrected text, simulating the thought process of performing the edit yourself.\n",
    "\n",
    "**Crucially:** Your reasoning process should *not* refer to or imply knowledge of a pre-existing target or corrected text. Focus *only* on applying the instruction to the source text to derive the necessary changes.\n",
    "\n",
    "Your entire output must consist *only* of the step-by-step reasoning process, enclosed within `<think></think>` tags. Do not include the original instruction, the source text, the final corrected text, or any other text outside these tags.\n",
    "\n",
    "**Example Interaction:**\n",
    "\n",
    "**User Input (constructed from your dataset):**\n",
    "\n",
    "```\n",
    "Source (src): Fix grammar in this sentence: If engineers do not come up with new ideas, they cannot find best solution for the problems.\n",
    "Target (tgt): If engineers do not come up with new ideas, they cannot find the best solution for different problems.\n",
    "```\n",
    "\n",
    "**Your Expected Output (demonstrating the reasoning to *derive* the correction):**\n",
    "\n",
    "<think>\n",
    "1. Instruction analysis: The task is to \"Fix grammar\". This requires identifying and correcting grammatical errors in the sentence \"If engineers do not come up with new ideas, they cannot find best solution for the problems.\"\n",
    "2. Sentence segment analysis: The first clause \"If engineers do not come up with new ideas\" appears grammatically sound.\n",
    "3. Focus on the second clause: \"they cannot find best solution for the problems.\"\n",
    "4. Identify potential error 1: The noun phrase \"best solution\". \"Best\" is a superlative adjective. Grammatical rule: Superlative adjectives modifying singular countable nouns generally require a definite article (\"the\").\n",
    "5. Apply correction 1: Insert \"the\" before \"best solution\". The phrase becomes \"the best solution\".\n",
    "6. Identify potential error/improvement area 2: The phrase \"for the problems\". While grammatically plausible, using \"the\" implies specific, previously identified problems. The context seems general. Improving grammatical flow or clarity might involve adjusting this.\n",
    "7. Consider alternatives for \"the problems\": \"for problems\" (too general?), \"for specific problems\" (adds info), \"for different problems\" (suggests variety, often fits general statements well).\n",
    "8. Apply correction 2: Replace \"the problems\" with \"different problems\" for better contextual fit and naturalness, aligning with the goal of general grammatical improvement and clarity often included in \"Fix grammar\" tasks.\n",
    "9. Synthesized correction based on reasoning: The corrected clause is \"they cannot find the best solution for different problems.\" The full sentence derived from applying the grammar fixes is \"If engineers do not come up with new ideas, they cannot find the best solution for different problems.\"\n",
    "</think>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded: 69071 rows\n"
     ]
    }
   ],
   "source": [
    "# --- Main Processing Logic ---\n",
    "if not gemini_api_key:\n",
    "    print(\"Error: GEMINI_API_KEY not found in environment variables.\")\n",
    "else:\n",
    "    # Initialize client\n",
    "    client = genai.Client(api_key=gemini_api_key)\n",
    "\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    try:\n",
    "        train_ds = load_dataset(\"grammarly/coedit\", split=\"train\", cache_dir=\"./data\")\n",
    "        print(f\"Dataset loaded: {len(train_ds)} rows\")\n",
    "        train_ds = train_ds.shuffle(seed=42)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not load dataset. {e}\")\n",
    "        train_ds = None # Ensure loop doesn't run\n",
    "\n",
    "# Global flag for controlled shutdown\n",
    "shutdown_requested = threading.Event()\n",
    "\n",
    "# Custom exception for controlled shutdown\n",
    "class APIShutdownException(Exception):\n",
    "    \"\"\"Exception raised to trigger a controlled shutdown of API processing.\"\"\"\n",
    "    pass        \n",
    "\n",
    "# --- Helper Function to Load Processed IDs ---\n",
    "\n",
    "def load_processed_ids(filename):\n",
    "    processed = set()\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    if '_id' in data:\n",
    "                        processed.add(data['_id'])\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping invalid JSON line in {filename}\")\n",
    "    print(f\"Loaded {len(processed)} processed IDs from {filename}\")\n",
    "    return processed\n",
    "\n",
    "# --- Worker Function ---\n",
    "\n",
    "def process_item(item, client, sys_prompt):\n",
    "    \"\"\"Processes a single dataset item to get reasoning.\"\"\"\n",
    "    item_id = item['_id']\n",
    "    \n",
    "    if shutdown_requested.is_set():\n",
    "        return {\"_id\": item_id, \"error\": \"Skipped due to shutdown request\"}\n",
    "\n",
    "    item_src = item['src']\n",
    "    item_tgt = item['tgt']\n",
    "    user_prompt = f\"Source (src): {item_src}\\nTarget (tgt): {item_tgt}\"\n",
    "\n",
    "    try:\n",
    "        # ---  API Call ---\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=user_prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "            system_instruction=sys_prompt,\n",
    "                    thinking_config=types.ThinkingConfig(\n",
    "                        thinking_budget=600 # 0 means no thinking None means no limit to thinking it can take however many tokens it wants.\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # --- Process Response ---\n",
    "        reasoning_text = response.text\n",
    "\n",
    "        if reasoning_text and reasoning_text.strip().startswith(\"```xml\") and reasoning_text.strip().endswith(\"```\"):\n",
    "            reasoning_text = reasoning_text[6:-3].strip() # removing the xml codefences\n",
    "\n",
    "        if reasoning_text.strip().startswith(\"<think>\") and reasoning_text.strip().endswith(\"</think>\"):\n",
    "            return {\"_id\": item_id, \"reasoning\": reasoning_text.strip()}\n",
    "        else:\n",
    "            reasoning_text = \"<think>\" + reasoning_text + \"</think>\"\n",
    "            return {\"_id\": item_id, \"reasoning\": reasoning_text.strip()}\n",
    "    except Exception as e:\n",
    "        if hasattr(e, 'code') and 400 <= e.code < 600:\n",
    "            error_msg = f\"Critical HTTP error {e.code}: {e}\"\n",
    "            print(f\"\\n{error_msg}\")\n",
    "            # Signal all threads to stop\n",
    "            shutdown_requested.set()\n",
    "            # Propagate the exception to interrupt processing\n",
    "            raise APIShutdownException(error_msg)\n",
    "        \n",
    "        # --- Handle API or Processing Errors ---\n",
    "        print(f\"\\nError processing ID {item_id}: {e}\")\n",
    "        return {\"_id\": item_id, \"error\": str(e)} # Return error indicator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 56264 processed IDs from ./data/train_reasoning.jsonl\n",
      "Filtering items to process...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcd1da3284044179f15a9f93f417de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering:   0%|          | 0/69071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12807 items needing processing.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2954b283322f41f9972b3884217512e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing items:   0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing item 1 of 499\n",
      "Processing item 2 of 499\n",
      "Processing item 3 of 499\n",
      "Processing item 4 of 499\n",
      "Processing item 5 of 499\n",
      "Processing item 6 of 499\n",
      "Processing item 7 of 499\n",
      "Processing item 8 of 499\n",
      "Processing item 9 of 499\n",
      "Processing item 10 of 499\n",
      "Processing item 11 of 499\n",
      "Processing item 12 of 499\n",
      "Processing item 13 of 499\n",
      "Processing item 14 of 499\n",
      "Processing item 15 of 499\n",
      "Processing item 16 of 499\n",
      "Processing item 17 of 499\n",
      "Processing item 18 of 499\n",
      "Processing item 19 of 499\n",
      "Processing item 20 of 499\n",
      "Processing item 21 of 499\n",
      "Processing item 22 of 499\n",
      "Processing item 23 of 499\n",
      "Processing item 24 of 499\n",
      "Processing item 25 of 499\n",
      "Processing item 26 of 499\n",
      "Processing item 27 of 499\n",
      "Processing item 28 of 499\n",
      "Processing item 29 of 499\n",
      "Processing item 30 of 499\n",
      "Processing item 31 of 499\n",
      "Processing item 32 of 499\n",
      "Processing item 33 of 499\n",
      "Processing item 34 of 499\n",
      "Processing item 35 of 499\n",
      "Processing item 36 of 499\n",
      "Processing item 37 of 499\n",
      "Processing item 38 of 499\n",
      "Processing item 39 of 499\n",
      "Processing item 40 of 499\n",
      "Processing item 41 of 499\n",
      "Processing item 42 of 499\n",
      "Processing item 43 of 499\n",
      "Processing item 44 of 499\n",
      "Processing item 45 of 499\n",
      "Processing item 46 of 499\n",
      "Processing item 47 of 499\n",
      "Processing item 48 of 499\n",
      "Processing item 49 of 499\n",
      "Processing item 50 of 499\n",
      "Processing item 51 of 499\n",
      "Processing item 52 of 499\n",
      "Processing item 53 of 499\n",
      "Processing item 54 of 499\n",
      "Processing item 55 of 499\n",
      "Processing item 56 of 499\n",
      "Processing item 57 of 499\n",
      "Processing item 58 of 499\n",
      "Processing item 59 of 499\n",
      "Processing item 60 of 499\n",
      "Processing item 61 of 499\n",
      "Processing item 62 of 499\n",
      "Processing item 63 of 499\n",
      "Processing item 64 of 499\n",
      "Processing item 65 of 499\n",
      "Processing item 66 of 499\n",
      "Processing item 67 of 499\n",
      "Processing item 68 of 499\n",
      "Processing item 69 of 499\n",
      "Processing item 70 of 499\n",
      "Processing item 71 of 499\n",
      "Processing item 72 of 499\n",
      "Processing item 73 of 499\n",
      "Processing item 74 of 499\n",
      "Processing item 75 of 499\n",
      "Processing item 76 of 499\n",
      "Processing item 77 of 499\n",
      "Processing item 78 of 499\n",
      "Processing item 79 of 499\n",
      "Processing item 80 of 499\n",
      "Processing item 81 of 499\n",
      "Processing item 82 of 499\n",
      "Processing item 83 of 499\n",
      "Processing item 84 of 499\n",
      "Processing item 85 of 499\n",
      "Processing item 86 of 499\n",
      "Processing item 87 of 499\n",
      "Processing item 88 of 499\n",
      "Processing item 89 of 499\n",
      "Processing item 90 of 499\n",
      "Processing item 91 of 499\n",
      "Processing item 92 of 499\n",
      "Processing item 93 of 499\n",
      "Processing item 94 of 499\n",
      "Processing item 95 of 499\n",
      "Processing item 96 of 499\n",
      "Processing item 97 of 499\n",
      "Processing item 98 of 499\n",
      "Processing item 99 of 499\n",
      "Processing item 100 of 499\n",
      "Processing item 101 of 499\n",
      "Processing item 102 of 499\n",
      "Processing item 103 of 499\n",
      "Processing item 104 of 499\n",
      "Processing item 105 of 499\n",
      "Processing item 106 of 499\n",
      "Processing item 107 of 499\n",
      "Processing item 108 of 499\n",
      "Processing item 109 of 499\n",
      "Processing item 110 of 499\n",
      "Processing item 111 of 499\n",
      "Processing item 112 of 499\n",
      "Processing item 113 of 499\n",
      "Processing item 114 of 499\n",
      "Processing item 115 of 499\n",
      "Processing item 116 of 499\n",
      "Processing item 117 of 499\n",
      "Processing item 118 of 499\n",
      "Processing item 119 of 499\n",
      "Processing item 120 of 499\n",
      "Processing item 121 of 499\n",
      "Processing item 122 of 499\n",
      "Processing item 123 of 499\n",
      "Processing item 124 of 499\n",
      "Processing item 125 of 499\n",
      "Processing item 126 of 499\n",
      "Processing item 127 of 499\n",
      "Processing item 128 of 499\n",
      "Processing item 129 of 499\n",
      "Processing item 130 of 499\n",
      "Processing item 131 of 499\n",
      "Processing item 132 of 499\n",
      "Processing item 133 of 499\n",
      "Processing item 134 of 499\n",
      "Processing item 135 of 499\n",
      "Processing item 136 of 499\n",
      "Processing item 137 of 499\n",
      "Processing item 138 of 499\n",
      "Processing item 139 of 499\n",
      "Processing item 140 of 499\n",
      "Processing item 141 of 499\n",
      "Processing item 142 of 499\n",
      "Processing item 143 of 499\n",
      "Processing item 144 of 499\n",
      "Processing item 145 of 499\n",
      "Processing item 146 of 499\n",
      "Processing item 147 of 499\n",
      "Processing item 148 of 499\n",
      "Processing item 149 of 499\n",
      "Processing item 150 of 499\n",
      "Processing item 151 of 499\n",
      "Processing item 152 of 499\n",
      "Processing item 153 of 499\n",
      "Processing item 154 of 499\n",
      "Processing item 155 of 499\n",
      "Processing item 156 of 499\n",
      "Processing item 157 of 499\n",
      "Processing item 158 of 499\n",
      "Processing item 159 of 499\n",
      "Processing item 160 of 499\n",
      "Processing item 161 of 499\n",
      "Processing item 162 of 499\n",
      "Processing item 163 of 499\n",
      "Processing item 164 of 499\n",
      "Processing item 165 of 499\n",
      "Processing item 166 of 499\n",
      "Processing item 167 of 499\n",
      "Processing item 168 of 499\n",
      "Processing item 169 of 499\n",
      "Processing item 170 of 499\n",
      "Processing item 171 of 499\n",
      "Processing item 172 of 499\n",
      "Processing item 173 of 499\n",
      "Processing item 174 of 499\n",
      "Processing item 175 of 499\n",
      "Processing item 176 of 499\n",
      "Processing item 177 of 499\n",
      "Processing item 178 of 499\n",
      "Processing item 179 of 499\n",
      "Processing item 180 of 499\n",
      "Processing item 181 of 499\n",
      "Processing item 182 of 499\n",
      "Processing item 183 of 499\n",
      "Processing item 184 of 499\n",
      "Processing item 185 of 499\n",
      "Processing item 186 of 499\n",
      "Processing item 187 of 499\n",
      "Processing item 188 of 499\n",
      "Processing item 189 of 499\n",
      "Processing item 190 of 499\n",
      "Processing item 191 of 499\n",
      "Processing item 192 of 499\n",
      "Processing item 193 of 499\n",
      "Processing item 194 of 499\n",
      "Processing item 195 of 499\n",
      "Processing item 196 of 499\n",
      "Processing item 197 of 499\n",
      "Processing item 198 of 499\n",
      "Processing item 199 of 499\n",
      "Processing item 200 of 499\n",
      "Processing item 201 of 499\n",
      "Processing item 202 of 499\n",
      "Processing item 203 of 499\n",
      "Processing item 204 of 499\n",
      "Processing item 205 of 499\n",
      "Processing item 206 of 499\n",
      "Processing item 207 of 499\n",
      "Processing item 208 of 499\n",
      "Processing item 209 of 499\n",
      "Processing item 210 of 499\n",
      "Processing item 211 of 499\n",
      "Processing item 212 of 499\n",
      "Processing item 213 of 499\n",
      "Processing item 214 of 499\n",
      "Processing item 215 of 499\n",
      "Processing item 216 of 499\n",
      "Processing item 217 of 499\n",
      "Processing item 218 of 499\n",
      "Processing item 219 of 499\n",
      "Processing item 220 of 499\n",
      "Processing item 221 of 499\n",
      "Processing item 222 of 499\n",
      "Processing item 223 of 499\n",
      "Processing item 224 of 499\n",
      "Processing item 225 of 499\n",
      "Processing item 226 of 499\n",
      "\n",
      "Critical HTTP error 500: 500 INTERNAL. {'error': {'code': 500, 'message': 'An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting', 'status': 'INTERNAL'}}\n"
     ]
    },
    {
     "ename": "APIShutdownException",
     "evalue": "Critical HTTP error 500: 500 INTERNAL. {'error': {'code': 500, 'message': 'An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting', 'status': 'INTERNAL'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m, in \u001b[0;36mprocess_item\u001b[0;34m(item, client, sys_prompt)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# ---  API Call ---\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m                \u001b[49m\u001b[43mthinking_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mThinkingConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mthinking_budget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 0 means no thinking None means no limit to thinking it can take however many tokens it wants.\u001b[39;49;00m\n\u001b[1;32m     64\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# --- Process Response ---\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/genai/models.py:5202\u001b[0m, in \u001b[0;36mModels.generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   5201\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5202\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m   5204\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5205\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAFC remote call \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is done.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/genai/models.py:4178\u001b[0m, in \u001b[0;36mModels._generate_content\u001b[0;34m(self, model, contents, config)\u001b[0m\n\u001b[1;32m   4176\u001b[0m request_dict \u001b[38;5;241m=\u001b[39m _common\u001b[38;5;241m.\u001b[39mencode_unserializable_types(request_dict)\n\u001b[0;32m-> 4178\u001b[0m response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[1;32m   4180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client\u001b[38;5;241m.\u001b[39mvertexai:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/genai/_api_client.py:755\u001b[0m, in \u001b[0;36mBaseApiClient.request\u001b[0;34m(self, http_method, path, request_dict, http_options)\u001b[0m\n\u001b[1;32m    752\u001b[0m http_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request(\n\u001b[1;32m    753\u001b[0m     http_method, path, request_dict, http_options\n\u001b[1;32m    754\u001b[0m )\n\u001b[0;32m--> 755\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    756\u001b[0m json_response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/genai/_api_client.py:684\u001b[0m, in \u001b[0;36mBaseApiClient._request\u001b[0;34m(self, http_request, stream)\u001b[0m\n\u001b[1;32m    677\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_httpx_client\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    678\u001b[0m     method\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    679\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    682\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mhttp_request\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[1;32m    683\u001b[0m )\n\u001b[0;32m--> 684\u001b[0m \u001b[43merrors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPIError\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[1;32m    686\u001b[0m     response\u001b[38;5;241m.\u001b[39mheaders, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response\u001b[38;5;241m.\u001b[39mtext]\n\u001b[1;32m    687\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/google/genai/errors.py:103\u001b[0m, in \u001b[0;36mAPIError.raise_for_response\u001b[0;34m(cls, response)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n\u001b[0;32m--> 103\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mServerError\u001b[0m: 500 INTERNAL. {'error': {'code': 500, 'message': 'An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting', 'status': 'INTERNAL'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAPIShutdownException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing item \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_CALLS_PER_RUN\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# to avoid rpm rate limit (10/min if you go over you incur costs)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitems_to_process\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(OUTPUT_FILE, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n",
      "Cell \u001b[0;32mIn[4], line 86\u001b[0m, in \u001b[0;36mprocess_item\u001b[0;34m(item, client, sys_prompt)\u001b[0m\n\u001b[1;32m     84\u001b[0m     shutdown_requested\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Propagate the exception to interrupt processing\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIShutdownException(error_msg)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# --- Handle API or Processing Errors ---\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mError processing ID \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAPIShutdownException\u001b[0m: Critical HTTP error 500: 500 INTERNAL. {'error': {'code': 500, 'message': 'An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting', 'status': 'INTERNAL'}}"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Main processing section ---\n",
    "if train_ds:\n",
    "    processed_ids = load_processed_ids(OUTPUT_FILE)\n",
    "    api_calls_made_this_run = 0\n",
    "    file_lock = threading.Lock()\n",
    "    items_to_process = []\n",
    "\n",
    "    print(\"Filtering items to process...\")\n",
    "    for item in tqdm(train_ds, desc=\"Filtering\"):\n",
    "        if item['_id'] not in processed_ids:\n",
    "            items_to_process.append(item)\n",
    "    print(f\"Found {len(items_to_process)} items needing processing.\")\n",
    "\n",
    "if MAX_CALLS_PER_RUN < 500:\n",
    "    for i in tqdm(range(MAX_CALLS_PER_RUN), desc=\"Processing items\", total=MAX_CALLS_PER_RUN):\n",
    "        print(f\"Processing item {i+1} of {MAX_CALLS_PER_RUN}\")\n",
    "        time.sleep(1) # to avoid rpm rate limit (10/min if you go over you incur costs)\n",
    "        response = process_item(items_to_process[i], client, sys_prompt)\n",
    "        if response and 'error' not in response:\n",
    "            with open(OUTPUT_FILE, 'a', encoding='utf-8') as outfile:\n",
    "                json_line = json.dumps(response)\n",
    "                outfile.write(json_line + '\\n')\n",
    "                outfile.flush()\n",
    "                processed_ids.add(response['_id'])\n",
    "        else:\n",
    "            print(f\"Error processing item {i} of {MAX_CALLS_PER_RUN}: {response['error'] if response else 'Unknown error'}\")\n",
    "\n",
    "elif MAX_CALLS_PER_RUN >= 500:\n",
    "    # Process items concurrently\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as outfile:\n",
    "        try:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                # stores ids of items that will be processed\n",
    "                futures = {}\n",
    "                \n",
    "                # Submit tasks only up to the daily limit\n",
    "                num_to_submit = min(len(items_to_process), MAX_CALLS_PER_RUN - api_calls_made_this_run)\n",
    "                print(f\"Submitting {num_to_submit} tasks to the executor...\")\n",
    "\n",
    "                submitted_count = 0\n",
    "                for i in range(len(items_to_process)):\n",
    "                    if submitted_count >= num_to_submit or shutdown_requested.is_set():\n",
    "                        break  # Stop submitting if we hit the limit or shutdown requested\n",
    "                    item = items_to_process[i]\n",
    "                    future = executor.submit(process_item, item, client, sys_prompt)\n",
    "                    futures[future] = item['_id']\n",
    "                    submitted_count += 1\n",
    "\n",
    "                print(f\"Tasks submitted ({submitted_count}). Waiting for results...\")\n",
    "\n",
    "                # Process results as they complete\n",
    "                completed_futures = []\n",
    "\n",
    "                for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing Results\"):\n",
    "                    if shutdown_requested.is_set():\n",
    "                        print(\"\\nShutdown requested. Canceling remaining tasks...\")\n",
    "                        for f in futures:\n",
    "                            if f not in completed_futures and not f.done():\n",
    "                                f.cancel()\n",
    "                        break\n",
    "        \n",
    "                    completed_futures.append(future)\n",
    "                    item_id = futures[future]       \n",
    "                    \n",
    "                    try:\n",
    "                        result_data = future.result()\n",
    "                        \n",
    "                        if result_data and \"error\" not in result_data:\n",
    "                            with file_lock:\n",
    "                                if result_data['_id'] not in processed_ids:\n",
    "                                    json_line = json.dumps(result_data)\n",
    "                                    outfile.write(json_line + '\\n')\n",
    "                                    outfile.flush()\n",
    "                                    processed_ids.add(result_data['_id'])\n",
    "                                    api_calls_made_this_run += 1\n",
    "                                    \n",
    "                    except APIShutdownException:\n",
    "                        print(f\"Task for item {item_id} aborted due to shutdown request\")\n",
    "                    except Exception as exc:\n",
    "                        print(f'\\nItem ID {item_id} generated an exception: {exc}')\n",
    "                        \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nKeyboard interrupt detected. Shutting down gracefully...\")\n",
    "            shutdown_requested.set()\n",
    "            \n",
    "    # Final status report\n",
    "    if shutdown_requested.is_set():\n",
    "        print(\"\\nProcessing terminated early due to errors or interruption.\")\n",
    "    else:\n",
    "        print(f\"\\nFinished processing batch. Made approximately {api_calls_made_this_run} successful API calls in this run.\")\n",
    "    \n",
    "    print(f\"Total processed IDs (including previous runs): {len(processed_ids)}\")\n",
    "\n",
    "else:\n",
    "    print(\"No items to process\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the token counts (use 0 if Thoughts tokens is None or zero):\n",
      "\n",
      "Calculating using THINKING output price ($3.50 / 1M tokens)\n",
      "\n",
      "--- Cost Breakdown ---\n",
      "Input Cost:       $0.000120\n",
      "Output Cost:      $0.003507 (based on 1002 billable output tokens)\n",
      "----------------------\n",
      "Total Estimated Cost: $0.003627\n",
      "\n",
      "Estimated Cost for 70,000 calls: $253.89\n"
     ]
    }
   ],
   "source": [
    "# Pricing per 1 Million tokens\n",
    "INPUT_PRICE_PER_MILLION = 0.15\n",
    "OUTPUT_NON_THINKING_PRICE_PER_MILLION = 0.60\n",
    "OUTPUT_THINKING_PRICE_PER_MILLION = 3.50\n",
    "\n",
    "# --- Get Token Counts from User ---\n",
    "print(\"Enter the token counts (use 0 if Thoughts tokens is None or zero):\")\n",
    "\n",
    "prompt_tokens = 800\n",
    "thoughts_tokens = 393 # Assume 0 if None was the actual value\n",
    "output_tokens = 609\n",
    "\n",
    "# --- Calculate Costs ---\n",
    "\n",
    "# Input Cost\n",
    "input_cost = (prompt_tokens / 1_000_000) * INPUT_PRICE_PER_MILLION\n",
    "\n",
    "# Output Cost (depends on whether thinking tokens were generated)\n",
    "if thoughts_tokens > 0:\n",
    "    # Thinking was used - price applies to output + thoughts tokens\n",
    "    billable_output_tokens = output_tokens + thoughts_tokens\n",
    "    output_cost = (billable_output_tokens / 1_000_000) * OUTPUT_THINKING_PRICE_PER_MILLION\n",
    "    print(\"\\nCalculating using THINKING output price ($3.50 / 1M tokens)\")\n",
    "else:\n",
    "    # No thinking - price applies only to output tokens\n",
    "    billable_output_tokens = output_tokens\n",
    "    output_cost = (billable_output_tokens / 1_000_000) * OUTPUT_NON_THINKING_PRICE_PER_MILLION\n",
    "    print(\"\\nCalculating using NON-THINKING output price ($0.60 / 1M tokens)\")\n",
    "\n",
    "# Total Cost\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "# --- Display Results ---\n",
    "print(f\"\\n--- Cost Breakdown ---\")\n",
    "print(f\"Input Cost:       ${input_cost:.6f}\")\n",
    "print(f\"Output Cost:      ${output_cost:.6f} (based on {billable_output_tokens} billable output tokens)\")\n",
    "print(f\"----------------------\")\n",
    "print(f\"Total Estimated Cost: ${total_cost:.6f}\")\n",
    "\n",
    "# Example calculation for 10,000 identical calls\n",
    "num_calls = 70000\n",
    "total_cost_10k = total_cost * num_calls\n",
    "print(f\"\\nEstimated Cost for {num_calls:,} calls: ${total_cost_10k:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "prompt = \"\"\"\n",
    "    what is the capital of new zealand?\n",
    "\"\"\"\n",
    "# The animal I'm thinking of is a platipus\n",
    "client = genai.Client(api_key=gemini_api_key)\n",
    "try:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-preview-04-17\",\n",
    "        # model=\"gemini-2.0-flash\",\n",
    "        contents=prompt\n",
    "    )\n",
    "except Exception as e:\n",
    "    if 400 <= e.code < 600:\n",
    "        print(f\"Exiting due to HTTP error {e.code}: {e}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of New Zealand is **Wellington**.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
