{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkOtQCVChDEd",
        "outputId": "0ecb58c8-8fd7-4bfc-fe24-19cbf3e58d8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2025 NVIDIA Corporation\n",
            "Built on Fri_Feb_21_20:23:50_PST_2025\n",
            "Cuda compilation tools, release 12.8, V12.8.93\n",
            "Build cuda_12.8.r12.8/compiler.35583870_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX70FSs3ylrY",
        "outputId": "473c3faf-2204-4757-a6f3-702fb036cbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WpWm-TzoK894",
        "outputId": "4fd6cc57-162f-40d1-8cb2-e0e25a65cf29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing libraries...\n"
          ]
        }
      ],
      "source": [
        "print(\"Installing libraries...\")\n",
        "!CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ciR9Ro0_y-zM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "%pip install errant huggingface-hub --quiet\n",
        "!python -m spacy download en_core_web_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KsvTYZWzLCq-"
      },
      "outputs": [],
      "source": [
        "from llama_cpp import Llama\n",
        "import os\n",
        "import urllib.request\n",
        "import subprocess\n",
        "import tempfile\n",
        "import re\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jDD9_oCo-Cd8"
      },
      "outputs": [],
      "source": [
        "# order matters: always substitute the longer N'T first, then the shorter endings\n",
        "CONTRACTIONS = [\"n't\", \"'ll\", \"'ve\", \"'re\", \"'d\", \"'m\", \"'s\"]\n",
        "\n",
        "# characters that carry a *leading* space in BEA tokenisation\n",
        "PUNCT = r\"\\.\\,\\!\\?\\:\\;\"\n",
        "\n",
        "def detokenize_bea(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert BEA‑style tokenisation to normal English prose.\n",
        "    \"\"\"\n",
        "    text = text.replace(\"’\", \"'\")\n",
        "    # 1. Join contractions\n",
        "    #   e.g. \"do n't\"  -> \"don't\"\n",
        "    #        \"she 'll\" -> \"she'll\"\n",
        "    # pattern:    <word> SP <contr>\n",
        "    for contr in CONTRACTIONS:\n",
        "        pattern = rf\"\\b(\\w+)\\s+{re.escape(contr)}\\b\"\n",
        "        repl    = rf\"\\1{contr}\"\n",
        "        text    = re.sub(pattern, repl, text)\n",
        "\n",
        "    # 2. Collapse space BEFORE punctuation  ( ... \"word .\" -> \"word.\" )\n",
        "    #  – only collapse if it’s a *single* normal blank, keep new‑lines\n",
        "    text = re.sub(r\" (?=[{}])\".format(PUNCT), \"\", text)\n",
        "\n",
        "    # 3. Clean spaces just inside brackets  : \"( word\" -> \"(word\"\n",
        "    text = re.sub(r\"\\(\\s+\", \"(\", text)\n",
        "    text = re.sub(r\"\\[\\s+\", \"[\", text)\n",
        "    text = re.sub(r\"\\s+\\)\", \")\", text)\n",
        "    text = re.sub(r\"\\s+\\]\", \"]\", text)\n",
        "\n",
        "    # 4. Collapse 2+ blanks into one\n",
        "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize_like_bea(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert normal English prose back to the peculiar BEA tokenisation.\n",
        "    (Not 100 % identical for every imaginable corner‑case, but round‑trips\n",
        "     correctly for the typical patterns in the dataset.)\n",
        "    \"\"\"\n",
        "    text = text.replace(\"’\", \"'\")\n",
        "    # 1. Split contractions      e.g. \"don't\" -> \"do n't\"\n",
        "    # n't must be done first because \"won't\" → \"wo n't\", not \"wo n' t\"\n",
        "    text = re.sub(r\"\\b(\\w+)n't\\b\",  r\"\\1 n't\", text)\n",
        "\n",
        "    # remaining endings\n",
        "    for contr in [\"'ll\", \"'ve\", \"'re\", \"'d\", \"'m\", \"'s\"]:\n",
        "        pattern = rf\"\\b(\\w+){re.escape(contr)}\\b\"\n",
        "        repl    = rf\"\\1 {contr}\"\n",
        "        text    = re.sub(pattern, repl, text)\n",
        "\n",
        "    # 2. Add a space *before* the main punctuation marks\n",
        "    # We first remove any existing spaces, then add exactly one.\n",
        "    text = re.sub(r\"\\s*([{}])\".format(PUNCT), r\" \\1\", text)\n",
        "\n",
        "    # 3. Remove extra double spaces that might have appeared\n",
        "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
        "\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EYSFuGEILFxQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/grammarly/pillars-of-gec/main/data/evaluation_sets/bea-dev.txt...\n",
            "File downloaded to: data/bea-dev.txt\n",
            "Downloading https://raw.githubusercontent.com/grammarly/pillars-of-gec/refs/heads/main/data/evaluation_sets/bea-dev.m2...\n",
            "File downloaded to: data/bea-dev.m2\n",
            "Read 4384 total sentences from data/bea-dev.txt\n",
            "Using a subset of 10 sentences for evaluation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Configuration ---\n",
        "MAX_EVAL_SAMPLES = 10 # Set to None to evaluate on the full dataset\n",
        "LOG_SAMPLE_OUTPUTS = 10 # Number of original/corrected pairs to print per model\n",
        "# GEC_PROOMPT = \"Fix grammaticality in this sentence:\"\n",
        "GEC_PROOMPT = \"\"\n",
        "\n",
        "# --- Loading data ---\n",
        "\n",
        "# Raw URL of the file\n",
        "bea_dev = \"https://raw.githubusercontent.com/grammarly/pillars-of-gec/main/data/evaluation_sets/bea-dev.txt\"\n",
        "bea_m2 = \"https://raw.githubusercontent.com/grammarly/pillars-of-gec/refs/heads/main/data/evaluation_sets/bea-dev.m2\"\n",
        "\n",
        "folder = \"data\"\n",
        "os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "dev_file_path = os.path.join(folder, \"bea-dev.txt\")\n",
        "if not os.path.exists(dev_file_path):\n",
        "    print(f\"Downloading {bea_dev}...\")\n",
        "    urllib.request.urlretrieve(bea_dev, dev_file_path)\n",
        "    print(f\"File downloaded to: {dev_file_path}\")\n",
        "else:\n",
        "    print(f\"Using existing file: {dev_file_path}\")\n",
        "\n",
        "\n",
        "m2_file_path = os.path.join(folder, \"bea-dev.m2\")\n",
        "if not os.path.exists(m2_file_path):\n",
        "    print(f\"Downloading {bea_m2}...\")\n",
        "    urllib.request.urlretrieve(bea_m2, m2_file_path)\n",
        "    print(f\"File downloaded to: {m2_file_path}\")\n",
        "else:\n",
        "     print(f\"Using existing file: {m2_file_path}\")\n",
        "\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def read_sentences(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "def create_subset_m2(input_m2_path, output_m2_path, num_sentences):\n",
        "    \"\"\"Reads an M2 file and writes the first num_sentences entries to a new file.\"\"\"\n",
        "    print(f\"Creating subset M2 file ({num_sentences} sentences) at: {output_m2_path}\")\n",
        "    sentence_count = 0\n",
        "    with open(input_m2_path, 'r', encoding='utf-8') as infile, \\\n",
        "         open(output_m2_path, 'w', encoding='utf-8') as outfile:\n",
        "        for line in infile:\n",
        "            # Ensure line is not empty before checking prefix\n",
        "            if line.strip() and line.startswith('S '):\n",
        "                sentence_count += 1\n",
        "                if sentence_count > num_sentences:\n",
        "                    break\n",
        "            # Check sentence_count before writing any line related to the current sentence block\n",
        "            if sentence_count <= num_sentences and sentence_count > 0: # Only write if we are within the limit and started processing sentences\n",
        "                 outfile.write(line)\n",
        "            # Handle edge case where the loop might end before writing the last newline of the final sentence block\n",
        "        if sentence_count <= num_sentences and sentence_count > 0:\n",
        "             outfile.write('\\n') # Ensure last entry ends properly if needed\n",
        "\n",
        "    print(f\"Finished creating subset M2 file.\")\n",
        "\n",
        "\n",
        "# --- Read Original Sentences (BEA Format) ---\n",
        "all_original_sentences_bea = read_sentences(dev_file_path)\n",
        "print(f\"Read {len(all_original_sentences_bea)} total sentences from {dev_file_path}\")\n",
        "\n",
        "# Select subset for evaluation\n",
        "if MAX_EVAL_SAMPLES is not None and MAX_EVAL_SAMPLES < len(all_original_sentences_bea):\n",
        "    original_sentences_bea = all_original_sentences_bea[:MAX_EVAL_SAMPLES]\n",
        "    print(f\"Using a subset of {len(original_sentences_bea)} sentences for evaluation.\")\n",
        "else:\n",
        "    original_sentences_bea = all_original_sentences_bea\n",
        "    print(f\"Using the full dataset of {len(original_sentences_bea)} sentences for evaluation.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ctSlvVFCLLFD"
      },
      "outputs": [],
      "source": [
        "class LLM:\n",
        "    def __init__(self, repo_id: str, filename: str, verbose: bool = False, n_gpu_layers=-1, gec_prompt:str = GEC_PROOMPT, **kwargs):\n",
        "        self.model_name = repo_id.split('/')[-1] # Extract model name for reporting\n",
        "        print(f\"Initializing {self.model_name}...\")\n",
        "        self.gec_prompt = gec_prompt\n",
        "        print(f\"Using gec prompt: {gec_prompt}\")\n",
        "        self.model = Llama.from_pretrained(\n",
        "            repo_id=repo_id,\n",
        "            filename=filename,\n",
        "            verbose=verbose,\n",
        "            n_gpu_layers=n_gpu_layers,\n",
        "            **kwargs\n",
        "        )\n",
        "        print(f\"{self.model_name} initialized.\")\n",
        "\n",
        "    # Takes NATURAL language prompts, returns NATURAL language results\n",
        "    def __call__(self, prompts: list[str], system_prompt: str = \"Correct *all* grammatical, punctuation, and spelling errors. Only output the corrected sentence.\", temperature: float = 0.2, max_tokens=200, **kwargs):\n",
        "        results = []\n",
        "        total_prompts = len(prompts)\n",
        "        for i, prompt in enumerate(prompts):\n",
        "            # Add progress indicator\n",
        "            print(f\"  Processing prompt {i+1}/{total_prompts}...\", end='\\r')\n",
        "            try:\n",
        "                response = self.model.create_chat_completion(\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": f\"{self.gec_prompt}+ {prompt}\"}\n",
        "                    ],\n",
        "                    temperature=temperature,\n",
        "                    max_tokens=max_tokens,\n",
        "                    **kwargs\n",
        "                )\n",
        "                # Handle potential empty responses or formatting issues\n",
        "                if response['choices'] and 'message' in response['choices'][0] and 'content' in response['choices'][0]['message']:\n",
        "                    correction = response['choices'][0]['message']['content'].strip()\n",
        "                    # Basic check for empty string, could be more robust\n",
        "                    if correction:\n",
        "                         results.append(correction)\n",
        "                    else:\n",
        "                        print(f\"\\nWarning: Model generated empty output for prompt: {prompt[:80]}...\") # Log snippet\n",
        "                        results.append(prompt) # Use original sentence as fallback\n",
        "                else:\n",
        "                    print(f\"\\nWarning: Unexpected response structure or no choices for prompt: {prompt[:80]}...\")\n",
        "                    results.append(prompt) # Fallback to original\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError during model inference for prompt: {prompt[:80]}...\\n{e}\")\n",
        "                # Fallback to original sentence on error to avoid crashing evaluation\n",
        "                results.append(prompt)\n",
        "        print(f\"\\n  Finished processing {total_prompts} prompts.\") # Newline after progress\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wden5wF5LOVN"
      },
      "outputs": [],
      "source": [
        "# --- Model Initialization ---\n",
        "# Define configurations instead of loading immediately\n",
        "model_configs = {\n",
        "    \"gemma-3-4b-it\": {\n",
        "        \"repo_id\": \"unsloth/gemma-3-4b-it-GGUF\",\n",
        "        \"filename\": \"*Q4_K_M.gguf\",\n",
        "        \"verbose\": False,\n",
        "        \"gec_prompt\": GEC_PROOMPT # Use the global GEC_PROOMPT\n",
        "    },\n",
        "    \"nomodit-4b-v0\": {\n",
        "        \"repo_id\": \"muzzz/nomodit-4b-merged-v0-Q4_K_M-GGUF\",\n",
        "        \"filename\": \"*q4_k_m.gguf\",\n",
        "        \"verbose\": False,\n",
        "        \"gec_prompt\": \"Fix grammaticality in this sentence:\" # Specific prompt for this model\n",
        "    },\n",
        "    \"nomodit-4b\": {\n",
        "        \"repo_id\": \"muzzz/nomodit-4b-merged-Q4_K_M-GGUF\",\n",
        "        \"filename\": \"*q4_k_m.gguf\",\n",
        "        \"verbose\": False,\n",
        "        \"gec_prompt\": \"Fix grammaticality in this sentence:\" # Specific prompt for this model\n",
        "    },\n",
        "    # Add configurations for commented-out models if you plan to use them later\n",
        "    # \"phi-4-mini-instruct\": {\"repo_id\": \"unsloth/Phi-4-mini-instruct-GGUF\", \"filename\": \"*Q4_K_M.gguf\", \"verbose\": False, \"gec_prompt\": GEC_PROOMPT},\n",
        "    # \"phi-4\": {\"repo_id\": \"unsloth/phi-4-GGUF\", \"filename\": \"*Q4_K_M.gguf\", \"verbose\": False, \"gec_prompt\": GEC_PROOMPT},\n",
        "    # \"llama-3.1-8B-Instruct\": {\"repo_id\": \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\", \"filename\": \"*Q4_K_M.gguf\", \"verbose\": False, \"gec_prompt\": GEC_PROOMPT},\n",
        "    # \"llama-3.2-3B-Instruct\": {\"repo_id\": \"unsloth/Llama-3.2-3B-Instruct-GGUF\", \"filename\": \"*Q4_K_M.gguf\", \"verbose\": False, \"gec_prompt\": GEC_PROOMPT},\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vZJ3KAmzLYNy"
      },
      "outputs": [],
      "source": [
        "# --- Benchmarking ---\n",
        "\n",
        "models_to_benchmark_names = [\n",
        "    \"gemma-3-4b-it\",\n",
        "    \"nomodit-4b-v0\",\n",
        "    \"nomodit-4b\",\n",
        "    # \"phi-4-mini-instruct\",\n",
        "    # \"phi-4\",\n",
        "    # \"llama-3.1-8B-Instruct\",\n",
        "    # \"llama-3.2-3B-Instruct\",\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Regex to capture P, R, F0.5 from errant_compare output\n",
        "# Updated regex to handle the table format output\n",
        "score_pattern = re.compile(\n",
        "    r\"=========== Span-Based Correction ============\\n\"  # Match header\n",
        "    r\"TP\\s+FP\\s+FN\\s+Prec\\s+Rec\\s+F0\\.5\\n\"             # Match table header row\n",
        "    r\"\\d+\\s+\\d+\\s+\\d+\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\\s+(\\d+\\.\\d+)\" # Capture Prec, Rec, F0.5 from data row\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KiURHm0HLZ11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Benchmark ---\n",
            "Detokenizing 10 sentences for LLM input...\n",
            "Detokenization complete.\n",
            "Creating subset M2 file (10 sentences) at: /tmp/tmptxa2vhcg.m2\n",
            "Finished creating subset M2 file.\n",
            "Using subset reference M2: /tmp/tmptxa2vhcg.m2\n",
            "\n",
            "===== Loading model: gemma-3-4b-it =====\n",
            "\n",
            "Initializing gemma-3-4b-it-GGUF...\n",
            "Using gec prompt: \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gemma-3-4b-it-GGUF initialized.\n",
            "\n",
            "===== Evaluating model: gemma-3-4b-it-GGUF =====\n",
            "Created temp orig (BEA format): /tmp/tmptfjdatea.txt\n",
            "Generating corrections for 10 sentences...\n",
            "  Processing prompt 10/10...\n",
            "  Finished processing 10 prompts.\n",
            "Generation took: 5.26 seconds\n",
            "Retokenizing LLM output to BEA format...\n",
            "Retokenization complete.\n",
            "\n",
            "--- Sample Outputs for gemma-3-4b-it-GGUF (First 10) ---\n",
            "Orig (BEA)  [1]: It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "Input (Nat) [1]: It's difficult answer at the question \" what are you going to do in the future? \" if the only one who has to know it is in two minds.\n",
            "Output (Nat)[1]: It is difficult to answer the question \"What are you going to do in the future?\" when only one person is unsure.\n",
            "Corr (BEA)  [1]: It is difficult to answer the question \"What are you going to do in the future ?\" when only one person is unsure .\n",
            "---\n",
            "Orig (BEA)  [2]: When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher .. I do n't know why .\n",
            "Input (Nat) [2]: When I was younger I used to say that I wanted to be a teacher, a saleswoman and even a butcher.. I don't know why.\n",
            "Output (Nat)[2]: When I was younger, I used to say that I wanted to be a teacher, a saleswoman, and even a butcher.\n",
            "Corr (BEA)  [2]: When I was younger , I used to say that I wanted to be a teacher , a saleswoman , and even a butcher .\n",
            "---\n",
            "Orig (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "Input (Nat) [3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Output (Nat)[3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Corr (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "---\n",
            "Orig (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "Input (Nat) [4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it easier.\n",
            "Output (Nat)[4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you will study it more easily.\n",
            "Corr (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you will study it more easily .\n",
            "---\n",
            "Orig (BEA)  [5]: Maybe I 'll change my mind , maybe not .\n",
            "Input (Nat) [5]: Maybe I'll change my mind, maybe not.\n",
            "Output (Nat)[5]: Maybe I'll change my mind, maybe not.\n",
            "Corr (BEA)  [5]: Maybe I 'll change my mind , maybe not .\n",
            "---\n",
            "Orig (BEA)  [6]: I think that the public transport will always be in the future .\n",
            "Input (Nat) [6]: I think that the public transport will always be in the future.\n",
            "Output (Nat)[6]: I think that public transport will always be important in the future.\n",
            "Corr (BEA)  [6]: I think that public transport will always be important in the future .\n",
            "---\n",
            "Orig (BEA)  [7]: The rich people will buy a car but the poor people always need to use a bus or taxi .\n",
            "Input (Nat) [7]: The rich people will buy a car but the poor people always need to use a bus or taxi.\n",
            "Output (Nat)[7]: The rich people will buy cars, but the poor people always need to use a bus or a taxi.\n",
            "Corr (BEA)  [7]: The rich people will buy cars , but the poor people always need to use a bus or a taxi .\n",
            "---\n",
            "Orig (BEA)  [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car .\n",
            "Input (Nat) [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car.\n",
            "Output (Nat)[8]: I consider it more convenient to drive a car because you can carry more things in your own car than when traveling by train.\n",
            "Corr (BEA)  [8]: I consider it more convenient to drive a car because you can carry more things in your own car than when traveling by train .\n",
            "---\n",
            "Orig (BEA)  [9]: Also , you 'll meet friendly people who usually ask to you something to be friends and change your telephone number .\n",
            "Input (Nat) [9]: Also, you'll meet friendly people who usually ask to you something to be friends and change your telephone number.\n",
            "Output (Nat)[9]: You'll also meet friendly people who usually ask you to do something to be friends and change your telephone number.\n",
            "Corr (BEA)  [9]: You 'll also meet friendly people who usually ask you to do something to be friends and change your telephone number .\n",
            "---\n",
            "Orig (BEA)  [10]: In my experience when I did n't have a car I used to use the bus to go to the school and go back to my house .\n",
            "Input (Nat) [10]: In my experience when I didn't have a car I used to use the bus to go to the school and go back to my house.\n",
            "Output (Nat)[10]: In my experience, when I didn't have a car, I used to use the bus to go to school and return home.\n",
            "Corr (BEA)  [10]: In my experience , when I did n't have a car , I used to use the bus to go to school and return home .\n",
            "---\n",
            "\n",
            "\n",
            "Created temp corrected (BEA format): /tmp/tmp6lnrsdu5.cor.txt\n",
            "Running errant_parallel... Orig: /tmp/tmptfjdatea.txt, Cor: /tmp/tmp6lnrsdu5.cor.txt, Out: /tmp/tmpk7awju9_.hyp.m2\n",
            "Running errant_compare... Hyp: /tmp/tmpk7awju9_.hyp.m2, Ref: /tmp/tmptxa2vhcg.m2\n",
            "Evaluation took: 2.68 seconds\n",
            "Scores for gemma-3-4b-it-GGUF (on 10 samples): P=0.2941, R=0.4, F0.5=0.3106\n",
            "Unloading model: gemma-3-4b-it-GGUF\n",
            "------------------------------\n",
            "\n",
            "===== Loading model: nomodit-4b-v0 =====\n",
            "\n",
            "Initializing nomodit-4b-merged-v0-Q4_K_M-GGUF...\n",
            "Using gec prompt: Fix grammaticality in this sentence:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nomodit-4b-merged-v0-Q4_K_M-GGUF initialized.\n",
            "\n",
            "===== Evaluating model: nomodit-4b-merged-v0-Q4_K_M-GGUF =====\n",
            "Created temp orig (BEA format): /tmp/tmp3xve29gw.txt\n",
            "Generating corrections for 10 sentences...\n",
            "  Processing prompt 10/10...\n",
            "  Finished processing 10 prompts.\n",
            "Generation took: 5.01 seconds\n",
            "Retokenizing LLM output to BEA format...\n",
            "Retokenization complete.\n",
            "\n",
            "--- Sample Outputs for nomodit-4b-merged-v0-Q4_K_M-GGUF (First 10) ---\n",
            "Orig (BEA)  [1]: It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "Input (Nat) [1]: It's difficult answer at the question \" what are you going to do in the future? \" if the only one who has to know it is in two minds.\n",
            "Output (Nat)[1]: It's a difficult answer to the question \"what are you going to do in the future?\" if the only one who has to know it is in two minds.\n",
            "Corr (BEA)  [1]: It 's a difficult answer to the question \"what are you going to do in the future ?\" if the only one who has to know it is in two minds .\n",
            "---\n",
            "Orig (BEA)  [2]: When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher .. I do n't know why .\n",
            "Input (Nat) [2]: When I was younger I used to say that I wanted to be a teacher, a saleswoman and even a butcher.. I don't know why.\n",
            "Output (Nat)[2]: When I was younger, I used to say that I wanted to be a teacher, a saleswoman, and even a butcher. I don't know why.\n",
            "Corr (BEA)  [2]: When I was younger , I used to say that I wanted to be a teacher , a saleswoman , and even a butcher . I do n't know why .\n",
            "---\n",
            "Orig (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "Input (Nat) [3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Output (Nat)[3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Corr (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "---\n",
            "Orig (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "Input (Nat) [4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it easier.\n",
            "Output (Nat)[4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it more easily.\n",
            "Corr (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it more easily .\n",
            "---\n",
            "Orig (BEA)  [5]: Maybe I 'll change my mind , maybe not .\n",
            "Input (Nat) [5]: Maybe I'll change my mind, maybe not.\n",
            "Output (Nat)[5]: Maybe I'll change my mind, maybe not.\n",
            "Corr (BEA)  [5]: Maybe I 'll change my mind , maybe not .\n",
            "---\n",
            "Orig (BEA)  [6]: I think that the public transport will always be in the future .\n",
            "Input (Nat) [6]: I think that the public transport will always be in the future.\n",
            "Output (Nat)[6]: I think that public transport will always be in the future.\n",
            "Corr (BEA)  [6]: I think that public transport will always be in the future .\n",
            "---\n",
            "Orig (BEA)  [7]: The rich people will buy a car but the poor people always need to use a bus or taxi .\n",
            "Input (Nat) [7]: The rich people will buy a car but the poor people always need to use a bus or taxi.\n",
            "Output (Nat)[7]: The rich people will buy a car, but the poor people always need to use a bus or taxi.\n",
            "Corr (BEA)  [7]: The rich people will buy a car , but the poor people always need to use a bus or taxi .\n",
            "---\n",
            "Orig (BEA)  [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car .\n",
            "Input (Nat) [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car.\n",
            "Output (Nat)[8]: I consider that it is more convenient to drive a car because you carry on more things in your own car than travelling by train.\n",
            "Corr (BEA)  [8]: I consider that it is more convenient to drive a car because you carry on more things in your own car than travelling by train .\n",
            "---\n",
            "Orig (BEA)  [9]: Also , you 'll meet friendly people who usually ask to you something to be friends and change your telephone number .\n",
            "Input (Nat) [9]: Also, you'll meet friendly people who usually ask to you something to be friends and change your telephone number.\n",
            "Output (Nat)[9]: Also, you'll meet friendly people who usually ask you something to be friends and change your telephone number.\n",
            "Corr (BEA)  [9]: Also , you 'll meet friendly people who usually ask you something to be friends and change your telephone number .\n",
            "---\n",
            "Orig (BEA)  [10]: In my experience when I did n't have a car I used to use the bus to go to the school and go back to my house .\n",
            "Input (Nat) [10]: In my experience when I didn't have a car I used to use the bus to go to the school and go back to my house.\n",
            "Output (Nat)[10]: In my experience, when I didn't have a car, I used to use the bus to go to school and go back to my house.\n",
            "Corr (BEA)  [10]: In my experience , when I did n't have a car , I used to use the bus to go to school and go back to my house .\n",
            "---\n",
            "\n",
            "\n",
            "Created temp corrected (BEA format): /tmp/tmpaa49lxse.cor.txt\n",
            "Running errant_parallel... Orig: /tmp/tmp3xve29gw.txt, Cor: /tmp/tmpaa49lxse.cor.txt, Out: /tmp/tmp0ccyazo4.hyp.m2\n",
            "Running errant_compare... Hyp: /tmp/tmp0ccyazo4.hyp.m2, Ref: /tmp/tmptxa2vhcg.m2\n",
            "Evaluation took: 1.80 seconds\n",
            "Scores for nomodit-4b-merged-v0-Q4_K_M-GGUF (on 10 samples): P=0.5, R=0.32, F0.5=0.4494\n",
            "Unloading model: nomodit-4b-merged-v0-Q4_K_M-GGUF\n",
            "------------------------------\n",
            "\n",
            "===== Loading model: nomodit-4b =====\n",
            "\n",
            "Initializing nomodit-4b-merged-Q4_K_M-GGUF...\n",
            "Using gec prompt: Fix grammaticality in this sentence:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nomodit-4b-merged-Q4_K_M-GGUF initialized.\n",
            "\n",
            "===== Evaluating model: nomodit-4b-merged-Q4_K_M-GGUF =====\n",
            "Created temp orig (BEA format): /tmp/tmpvekg2up7.txt\n",
            "Generating corrections for 10 sentences...\n",
            "  Processing prompt 10/10...\n",
            "  Finished processing 10 prompts.\n",
            "Generation took: 5.12 seconds\n",
            "Retokenizing LLM output to BEA format...\n",
            "Retokenization complete.\n",
            "\n",
            "--- Sample Outputs for nomodit-4b-merged-Q4_K_M-GGUF (First 10) ---\n",
            "Orig (BEA)  [1]: It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "Input (Nat) [1]: It's difficult answer at the question \" what are you going to do in the future? \" if the only one who has to know it is in two minds.\n",
            "Output (Nat)[1]: It's a difficult answer to the question \"What are you going to do in the future?\" if the only one who has to know it is in two minds.\n",
            "Corr (BEA)  [1]: It 's a difficult answer to the question \"What are you going to do in the future ?\" if the only one who has to know it is in two minds .\n",
            "---\n",
            "Orig (BEA)  [2]: When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher .. I do n't know why .\n",
            "Input (Nat) [2]: When I was younger I used to say that I wanted to be a teacher, a saleswoman and even a butcher.. I don't know why.\n",
            "Output (Nat)[2]: When I was younger I used to say that I wanted to be a teacher, a saleswoman, and even a butcher. I don't know why.\n",
            "Corr (BEA)  [2]: When I was younger I used to say that I wanted to be a teacher , a saleswoman , and even a butcher . I do n't know why .\n",
            "---\n",
            "Orig (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "Input (Nat) [3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Output (Nat)[3]: I would like to study Psychology because one day I would open my own psychology office and help people.\n",
            "Corr (BEA)  [3]: I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "---\n",
            "Orig (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "Input (Nat) [4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it easier.\n",
            "Output (Nat)[4]: It's difficult because I'll have to study hard and a lot, but I think that if you like a subject, you'll study it more easily.\n",
            "Corr (BEA)  [4]: It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it more easily .\n",
            "---\n",
            "Orig (BEA)  [5]: Maybe I 'll change my mind , maybe not .\n",
            "Input (Nat) [5]: Maybe I'll change my mind, maybe not.\n",
            "Output (Nat)[5]: Maybe I'll change my mind, maybe I won't.\n",
            "Corr (BEA)  [5]: Maybe I 'll change my mind , maybe I wo n't .\n",
            "---\n",
            "Orig (BEA)  [6]: I think that the public transport will always be in the future .\n",
            "Input (Nat) [6]: I think that the public transport will always be in the future.\n",
            "Output (Nat)[6]: I think that public transport will always be in the future.\n",
            "Corr (BEA)  [6]: I think that public transport will always be in the future .\n",
            "---\n",
            "Orig (BEA)  [7]: The rich people will buy a car but the poor people always need to use a bus or taxi .\n",
            "Input (Nat) [7]: The rich people will buy a car but the poor people always need to use a bus or taxi.\n",
            "Output (Nat)[7]: The rich people will buy a car, but the poor people always need to use a bus or taxi.\n",
            "Corr (BEA)  [7]: The rich people will buy a car , but the poor people always need to use a bus or taxi .\n",
            "---\n",
            "Orig (BEA)  [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car .\n",
            "Input (Nat) [8]: I consider that is more convenient to drive a car because you carry on more things in your own car than travelling by car.\n",
            "Output (Nat)[8]: I consider that it is more convenient to drive a car because you carry on more things in your own car than travelling by car.\n",
            "Corr (BEA)  [8]: I consider that it is more convenient to drive a car because you carry on more things in your own car than travelling by car .\n",
            "---\n",
            "Orig (BEA)  [9]: Also , you 'll meet friendly people who usually ask to you something to be friends and change your telephone number .\n",
            "Input (Nat) [9]: Also, you'll meet friendly people who usually ask to you something to be friends and change your telephone number.\n",
            "Output (Nat)[9]: Also, you'll meet friendly people who usually ask you something to be friends and change your telephone number.\n",
            "Corr (BEA)  [9]: Also , you 'll meet friendly people who usually ask you something to be friends and change your telephone number .\n",
            "---\n",
            "Orig (BEA)  [10]: In my experience when I did n't have a car I used to use the bus to go to the school and go back to my house .\n",
            "Input (Nat) [10]: In my experience when I didn't have a car I used to use the bus to go to the school and go back to my house.\n",
            "Output (Nat)[10]: In my experience, when I didn't have a car, I used to take the bus to go to school and go back to my house.\n",
            "Corr (BEA)  [10]: In my experience , when I did n't have a car , I used to take the bus to go to school and go back to my house .\n",
            "---\n",
            "\n",
            "\n",
            "Created temp corrected (BEA format): /tmp/tmpk64vxdoz.cor.txt\n",
            "Running errant_parallel... Orig: /tmp/tmpvekg2up7.txt, Cor: /tmp/tmpk64vxdoz.cor.txt, Out: /tmp/tmpn3i7uu_v.hyp.m2\n",
            "Running errant_compare... Hyp: /tmp/tmpn3i7uu_v.hyp.m2, Ref: /tmp/tmptxa2vhcg.m2\n",
            "Evaluation took: 1.95 seconds\n",
            "Scores for nomodit-4b-merged-Q4_K_M-GGUF (on 10 samples): P=0.4118, R=0.28, F0.5=0.3763\n",
            "Unloading model: nomodit-4b-merged-Q4_K_M-GGUF\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n--- Starting Benchmark ---\")\n",
        "\n",
        "# --- Prepare Input Data --- #\n",
        "# Detokenize the input sentences ONCE before the loop\n",
        "print(f\"Detokenizing {len(original_sentences_bea)} sentences for LLM input...\")\n",
        "natural_input_sentences = [detokenize_bea(s) for s in original_sentences_bea]\n",
        "print(\"Detokenization complete.\")\n",
        "\n",
        "# --- Create Reference M2 Subset (if needed) --- #\n",
        "reference_m2_to_use = m2_file_path\n",
        "temp_ref_m2_subset_path = None # Initialize path variable\n",
        "\n",
        "if MAX_EVAL_SAMPLES is not None and MAX_EVAL_SAMPLES < len(all_original_sentences_bea):\n",
        "    # Create a temporary file path that persists until explicitly deleted\n",
        "    temp_ref_m2_file = tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".m2\", encoding='utf-8')\n",
        "    temp_ref_m2_subset_path = temp_ref_m2_file.name\n",
        "    temp_ref_m2_file.close() # Close the file handle immediately, but keep the file\n",
        "    create_subset_m2(m2_file_path, temp_ref_m2_subset_path, len(original_sentences_bea))\n",
        "    reference_m2_to_use = temp_ref_m2_subset_path\n",
        "    print(f\"Using subset reference M2: {reference_m2_to_use}\")\n",
        "else:\n",
        "    print(f\"Using full reference M2: {reference_m2_to_use}\")\n",
        "\n",
        "\n",
        "for model_key in models_to_benchmark_names:\n",
        "    config = model_configs.get(model_key)\n",
        "    if not config:\n",
        "        print(f\"Warning: Model key '{model_key}' not found in model configurations. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Initialize LLM object here, inside the loop\n",
        "    print(f\"\\n===== Loading model: {model_key} =====\\n\")\n",
        "    llm = None # Initialize llm to None\n",
        "    try:\n",
        "        llm = LLM(**config) # Load the model using its specific config\n",
        "        model_name = llm.model_name # Use name from LLM object\n",
        "        print(f\"\\n===== Evaluating model: {model_name} =====\")\n",
        "\n",
        "        # Paths for temporary files specific to this model iteration\n",
        "        temp_orig_bea_path = None\n",
        "        temp_cor_bea_path = None\n",
        "        temp_hyp_m2_path = None\n",
        "\n",
        "        # --- Create temporary file for ORIGINAL sentences (BEA format) --- #\n",
        "        # This file is needed as -orig input for errant_parallel\n",
        "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".txt\", encoding='utf-8') as temp_orig_file:\n",
        "            for sentence in original_sentences_bea: # Write the BEA format sentences\n",
        "                temp_orig_file.write(sentence + '\\n')\n",
        "            temp_orig_bea_path = temp_orig_file.name\n",
        "\n",
        "        # --- Run Model (using detokenized input) --- #\n",
        "        start_time = time.time()\n",
        "        print(f\"Generating corrections for {len(natural_input_sentences)} sentences...\")\n",
        "        # Pass the NATURAL language sentences to the LLM\n",
        "        corrected_sentences_natural = llm(natural_input_sentences)\n",
        "        generation_time = time.time() - start_time\n",
        "        print(f\"Generation took: {generation_time:.2f} seconds\")\n",
        "\n",
        "        # --- Retokenize Output --- #\n",
        "        print(\"Retokenizing LLM output to BEA format...\")\n",
        "        corrected_sentences_bea = [tokenize_like_bea(s) for s in corrected_sentences_natural]\n",
        "        print(\"Retokenization complete.\")\n",
        "\n",
        "\n",
        "        # --- Log Sample Outputs --- #\n",
        "        if LOG_SAMPLE_OUTPUTS > 0:\n",
        "            print(f\"\\n--- Sample Outputs for {model_name} (First {min(LOG_SAMPLE_OUTPUTS, len(original_sentences_bea))}) ---\")\n",
        "            for i in range(min(LOG_SAMPLE_OUTPUTS, len(original_sentences_bea))):\n",
        "                print(f\"Orig (BEA)  [{i+1}]: {original_sentences_bea[i]}\")\n",
        "                # print(f\"Input (Nat) [{i+1}]: {natural_input_sentences[i]}\") # Optional: print detokenized input\n",
        "                # print(f\"Output (Nat)[{i+1}]: {corrected_sentences_natural[i]}\") # Optional: print natural output\n",
        "                print(f\"Corr (BEA)  [{i+1}]: {corrected_sentences_bea[i]}\")\n",
        "                print(\"---\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "        # --- Evaluation --- #\n",
        "        if len(corrected_sentences_bea) != len(original_sentences_bea):\n",
        "            print(f\"Error: Number of corrected sentences ({len(corrected_sentences_bea)}) does not match original ({len(original_sentences_bea)}) for model {model_name}. Skipping evaluation.\")\n",
        "            results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"Length Mismatch\", \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": \"N/A\"}\n",
        "            # No 'continue' here, need to go to finally block\n",
        "\n",
        "        else: # Only run eval if lengths match\n",
        "            eval_start_time = time.time()\n",
        "\n",
        "            # 3. Write RETOKENIZED CORRECTED sentences (BEA format) to a temporary file\n",
        "            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".cor.txt\", encoding='utf-8') as temp_cor_file:\n",
        "                for sentence in corrected_sentences_bea: # Write the BEA format sentences\n",
        "                    temp_cor_file.write(sentence + '\\n')\n",
        "                temp_cor_bea_path = temp_cor_file.name\n",
        "            print(f\"Created temp corrected (BEA format): {temp_cor_bea_path}\")\n",
        "\n",
        "\n",
        "            # 4. Create hypothesis M2 file using errant_parallel\n",
        "            #    Input: -orig (BEA format), -cor (BEA format)\n",
        "            #    Output: hypothesis M2 (ERRANT will likely re-tokenize internally based on spaCy,\n",
        "            #            but we feed it BEA format for consistency at this step)\n",
        "            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix=\".hyp.m2\") as temp_hyp_m2_file:\n",
        "                temp_hyp_m2_path = temp_hyp_m2_file.name\n",
        "\n",
        "            print(f\"Running errant_parallel... Orig: {temp_orig_bea_path}, Cor: {temp_cor_bea_path}, Out: {temp_hyp_m2_path}\")\n",
        "            parallel_cmd = [\"errant_parallel\", \"-orig\", temp_orig_bea_path, \"-cor\", temp_cor_bea_path, \"-out\", temp_hyp_m2_path]\n",
        "            parallel_result = subprocess.run(parallel_cmd, capture_output=True, text=True, check=False)\n",
        "\n",
        "            if parallel_result.returncode != 0:\n",
        "                print(f\"Error running errant_parallel for {model_name}:\")\n",
        "                print(f\"Stderr: {parallel_result.stderr}\")\n",
        "                results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"errant_parallel failed\", \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": \"N/A\"}\n",
        "                # No 'continue' here, need to go to finally block\n",
        "\n",
        "            elif \"Processing 0 sentences\" in parallel_result.stderr: # Check if ERRANT processed anything\n",
        "                 print(f\"Warning: errant_parallel reported processing 0 sentences. Check input files or ERRANT setup.\")\n",
        "                 print(f\"Stderr: {parallel_result.stderr}\")\n",
        "                 # Decide whether to continue or mark as error - currently just warns\n",
        "                 results[model_name] = {\"P\": \"N/A\", \"R\": \"N/A\", \"F0.5\": \"ERRANT processed 0\", \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": \"N/A\"}\n",
        "\n",
        "            else:\n",
        "                # 5. Compare hypothesis M2 with reference M2 using errant_compare\n",
        "                #    Input: -hyp (Generated M2), -ref (BEA Reference M2 - potentially subsetted)\n",
        "                print(f\"Running errant_compare... Hyp: {temp_hyp_m2_path}, Ref: {reference_m2_to_use}\")\n",
        "                compare_cmd = [\"errant_compare\", \"-hyp\", temp_hyp_m2_path, \"-ref\", reference_m2_to_use]\n",
        "                compare_result = subprocess.run(compare_cmd, capture_output=True, text=True, check=False)\n",
        "\n",
        "                if compare_result.returncode != 0:\n",
        "                    print(f\"Error running errant_compare for {model_name}:\")\n",
        "                    print(f\"Stderr: {compare_result.stderr}\")\n",
        "                    results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"errant_compare failed\", \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": \"N/A\"}\n",
        "                    # No 'continue' here, need to go to finally block\n",
        "                else:\n",
        "                    # 6. Parse scores\n",
        "                    eval_time = time.time() - eval_start_time\n",
        "                    print(f\"Evaluation took: {eval_time:.2f} seconds\")\n",
        "                    # print(f\"errant_compare output for {model_name}:\\n{compare_result.stdout}\") # Uncomment for debugging\n",
        "\n",
        "                    match = score_pattern.search(compare_result.stdout)\n",
        "                    if match:\n",
        "                        p, r, f05 = match.groups()\n",
        "                        print(f\"Scores for {model_name} (on {len(original_sentences_bea)} samples): P={p}, R={r}, F0.5={f05}\")\n",
        "                        results[model_name] = {\n",
        "                            \"P\": float(p),\n",
        "                            \"R\": float(r),\n",
        "                            \"F0.5\": float(f05),\n",
        "                            \"Samples\": len(original_sentences_bea),\n",
        "                            \"Generation Time (s)\": f\"{generation_time:.2f}\",\n",
        "                            \"Eval Time (s)\": f\"{eval_time:.2f}\"\n",
        "                        }\n",
        "                    else:\n",
        "                        print(f\"Could not parse scores from errant_compare output for {model_name}.\")\n",
        "                        print(f\"Output:\\n{compare_result.stdout}\")\n",
        "                        results[model_name] = {\"P\": \"Error\", \"R\": \"Error\", \"F0.5\": \"Parsing failed\", \"Samples\": len(original_sentences_bea), \"Generation Time (s)\": f\"{generation_time:.2f}\", \"Eval Time (s)\": f\"{eval_time:.2f}\"}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during loading or evaluation for {model_key}: {e}\")\n",
        "        # Use model_key if llm object wasn't successfully created\n",
        "        current_model_name = llm.model_name if llm else model_key\n",
        "        gen_time_str = f\"{generation_time:.2f}\" if 'generation_time' in locals() else \"N/A\"\n",
        "        eval_time_str = f\"{eval_time:.2f}\" if 'eval_time' in locals() else \"N/A\"\n",
        "\n",
        "        if 'eval_start_time' in locals():\n",
        "             current_eval_time = time.time() - eval_start_time\n",
        "             eval_time_str = f\"{current_eval_time:.2f}\"\n",
        "        elif 'start_time' in locals():\n",
        "             current_gen_time = time.time() - start_time\n",
        "             gen_time_str = f\"{current_gen_time:.2f}\"\n",
        "\n",
        "\n",
        "        results[current_model_name] = {\n",
        "             \"P\": \"Error\", \"R\": \"Error\", \"F0.5\": f\"Exception: {type(e).__name__}\",\n",
        "             \"Samples\": len(original_sentences_bea) if 'original_sentences_bea' in locals() else 'N/A',\n",
        "             \"Generation Time (s)\": gen_time_str,\n",
        "             \"Eval Time (s)\": eval_time_str\n",
        "         }\n",
        "    finally:\n",
        "        # 7. Clean up temporary files created in THIS iteration\n",
        "        for path in [temp_orig_bea_path, temp_cor_bea_path, temp_hyp_m2_path]:\n",
        "            if path and os.path.exists(path):\n",
        "                try:\n",
        "                    os.remove(path)\n",
        "                except OSError as e:\n",
        "                    print(f\"  Error removing temp file {path}: {e}\")\n",
        "\n",
        "        # 8. Explicitly delete the LLM object to free VRAM\n",
        "        if llm is not None:\n",
        "            print(f\"Unloading model: {llm.model_name}\")\n",
        "            del llm\n",
        "            # Optional: Force garbage collection if memory issues persist\n",
        "            # import gc\n",
        "            # gc.collect()\n",
        "            # If llama-cpp-python uses torch backend implicitly:\n",
        "            # try:\n",
        "            #     import torch\n",
        "            #     torch.cuda.empty_cache()\n",
        "            # except ImportError:\n",
        "            #     pass # Ignore if torch isn't installed\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "\n",
        "# --- Final Cleanup for Subset Reference M2 ---\n",
        "if temp_ref_m2_subset_path and os.path.exists(temp_ref_m2_subset_path):\n",
        "     try:\n",
        "         os.remove(temp_ref_m2_subset_path)\n",
        "     except OSError as e:\n",
        "         print(f\"  Error removing {temp_ref_m2_subset_path}: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SVgMpCSULdxZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Benchmark Results ---\n",
            "Model: gemma-3-4b-it-GGUF\n",
            "  F0.5: 0.3106\n",
            "  P: 0.2941\n",
            "  R: 0.4000\n",
            "  Samples: 10\n",
            "  Generation Time (s): 5.26\n",
            "  Eval Time (s): 2.68\n",
            "--------------------\n",
            "Model: nomodit-4b-merged-Q4_K_M-GGUF\n",
            "  F0.5: 0.3763\n",
            "  P: 0.4118\n",
            "  R: 0.2800\n",
            "  Samples: 10\n",
            "  Generation Time (s): 5.12\n",
            "  Eval Time (s): 1.95\n",
            "--------------------\n",
            "Model: nomodit-4b-merged-v0-Q4_K_M-GGUF\n",
            "  F0.5: 0.4494\n",
            "  P: 0.5000\n",
            "  R: 0.3200\n",
            "  Samples: 10\n",
            "  Generation Time (s): 5.01\n",
            "  Eval Time (s): 1.80\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"\\n--- Benchmark Results ---\")\n",
        "# Sort results alphabetically by model name for consistent output\n",
        "sorted_results = dict(sorted(results.items()))\n",
        "\n",
        "for model_name, scores in sorted_results.items():\n",
        "    print(f\"Model: {model_name}\")\n",
        "    # Define a preferred order for metrics if desired\n",
        "    metric_order = [\"F0.5\", \"P\", \"R\", \"Samples\", \"Generation Time (s)\", \"Eval Time (s)\"]\n",
        "    for metric in metric_order:\n",
        "        if metric in scores:\n",
        "            value = scores[metric]\n",
        "            # Format floats nicely\n",
        "            if isinstance(value, float):\n",
        "                print(f\"  {metric}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"  {metric}: {value}\")\n",
        "    # Print any other metrics not in the preferred order (e.g., error messages)\n",
        "    for metric, value in scores.items():\n",
        "        if metric not in metric_order:\n",
        "             print(f\"  {metric}: {value}\")\n",
        "    print(\"-\" * 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STtaG0e4LgQP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
